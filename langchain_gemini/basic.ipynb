{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e7d47d",
   "metadata": {},
   "source": [
    "### langchain with gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacdfec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e301b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GEMINI_API_KEY']= os.getenv('GEMINI_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY']= os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'True'\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bd8be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User1\\anaconda3\\envs\\langchain-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8fa6c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.5-pro', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000019C6B7B18A0>, default_metadata=())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d52e744c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Of course. Here is a comprehensive explanation of Generative AI, broken down for easy understanding.\\n\\n### The Simple Definition\\n\\nAt its core, **Generative AI is a type of artificial intelligence that can create new, original content.**\\n\\nThink of it not as an AI that just analyzes or categorizes information, but as one that *creates* something that didn\\'t exist before. This new content can be in the form of text, images, music, code, or even video.\\n\\nA simple analogy:\\n*   **Traditional AI** is like a student who can ace a multiple-choice test (classifying, identifying, predicting).\\n*   **Generative AI** is like a student who can write a brand-new essay or paint an original picture (creating, generating).\\n\\n---\\n\\n### How Does It Work?\\n\\nGenerative AI works by learning from enormous amounts of existing data. It\\'s trained on vast libraries of books, articles, websites, images, and code. During this training process, it doesn\\'t just memorize the data; it learns the underlying **patterns, structures, styles, and relationships** within it.\\n\\n1.  **Training:** The AI model (often a type of neural network called a **Transformer**) is fed massive datasets. For example, a text model like ChatGPT was trained on a huge portion of the internet\\'s text.\\n2.  **Learning Patterns:** The model learns grammar, facts, reasoning styles, and artistic styles. It learns that the word \"sky\" is often associated with \"blue,\" and that a Monet painting has a certain brushstroke style.\\n3.  **Generating:** When you give it a prompt (a command or question), it uses its learned patterns to predict the most likely sequence of what should come next. It generates content piece by piece (e.g., word by word, or pixel by pixel) based on the context you\\'ve provided, creating something new that fits the pattern.\\n\\nIt\\'s like a super-powered autocomplete. If you type \"The first person to walk on the moon was...\", it predicts the most probable next word is \"Neil,\" followed by \"Armstrong,\" because it has learned this fact from millions of texts. But it can do this for entire paragraphs, poems, and complex ideas.\\n\\n---\\n\\n### What Can Generative AI Create? (Examples)\\n\\nGenerative AI isn\\'t just one thing; it\\'s a category. Here are the most common types:\\n\\n*   **Text Generation:** Creating emails, articles, poems, marketing copy, and computer code.\\n    *   **Examples:** **ChatGPT** (from OpenAI), **Gemini** (from Google), **Claude** (from Anthropic).\\n\\n*   **Image Generation:** Creating realistic photos, paintings, and illustrations from a text description.\\n    *   **Examples:** **Midjourney**, **DALL-E 3** (from OpenAI), **Stable Diffusion**.\\n\\n*   **Audio and Music Generation:** Composing music in various genres, creating voiceovers, or generating sound effects.\\n    *   **Examples:** **Suno** (for music), **ElevenLabs** (for voice).\\n\\n*   **Video Generation:** Creating short video clips from text prompts, though this technology is still emerging.\\n    *   **Examples:** **Sora** (from OpenAI), **RunwayML**.\\n\\n*   **Code Generation:** Writing functions, debugging code, and even creating entire applications based on a description.\\n    *   **Examples:** **GitHub Copilot**.\\n\\n---\\n\\n### Key Applications (Where is it used?)\\n\\nGenerative AI is already being integrated into many fields:\\n\\n*   **Content Creation:** Writers and marketers use it to brainstorm ideas and draft articles.\\n*   **Design and Art:** Artists and designers use it for inspiration and to create concept art.\\n*   **Software Development:** Programmers use it to write and debug code faster.\\n*   **Entertainment:** Used in game development for creating characters and environments, and in film for special effects.\\n*   **Customer Service:** Powering more sophisticated and human-like chatbots.\\n*   **Scientific Research:** Simulating complex systems, like protein folding, to accelerate drug discovery.\\n\\n---\\n\\n### Challenges and Concerns\\n\\nWhile powerful, Generative AI also has significant challenges:\\n\\n*   **Accuracy and \"Hallucinations\":** The AI can confidently state incorrect information as fact. It\\'s \"making things up\" that sound plausible.\\n*   **Bias:** If the training data contains biases (e.g., racial or gender stereotypes), the AI will learn and reproduce them.\\n*   **Ethical Use:** The potential for misuse is high, including creating deepfakes, spreading misinformation, and automating plagiarism.\\n*   **Copyright and Ownership:** Who owns AI-generated content? The person who wrote the prompt, the company that made the AI, or no one? This is a major legal gray area.\\n*   **Job Displacement:** There are concerns about its impact on creative and knowledge-based jobs.\\n\\n### In Summary\\n\\n**Generative AI is a groundbreaking technology that shifts the role of computers from just processing information to creating it. It\\'s a powerful creative partner and productivity tool, but one that must be used with a critical and responsible mindset.**', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--6f8d37ff-48cb-46bc-a2c8-4d9b9fcd9b2f-0', usage_metadata={'input_tokens': 6, 'output_tokens': 1099, 'total_tokens': 2472, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result = llm.invoke(\"what is generative ai?\")\n",
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06462049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Of course! \"What is a cat?\" is a simple question with a wonderfully complex answer.\\n\\nAt its core, a **cat** (scientific name: *Felis catus*) is a small, carnivorous mammal that is most commonly kept as a domestic pet. However, they are much more than that. They are a fascinating blend of a skilled predator and an affectionate companion.\\n\\nHereâ€™s a breakdown of what a cat is from different perspectives:\\n\\n---\\n\\n### 1. The Biological Answer (The Predator)\\n\\nFrom a scientific standpoint, a cat is a highly evolved hunting machine.\\n\\n*   **Family:** Cats belong to the **Felidae** family, which also includes lions, tigers, cheetahs, and jaguars. Domestic cats share many of the same predatory instincts and physical traits as their wild relatives.\\n*   **Physical Traits:**\\n    *   **Flexible Body:** They have a strong, muscular, and incredibly flexible body, allowing them to jump, climb, and squeeze through tight spaces.\\n    *   **Sharp Claws:** Their retractable claws stay sharp for hunting, climbing, and self-defense.\\n    *   **Keen Senses:** They have exceptional hearing (able to hear high-frequency sounds made by rodents), excellent night vision, and a powerful sense of smell.\\n    *   **Whiskers:** Their whiskers are not just hair; they are sensitive tactile sensors that help them navigate in the dark and determine if they can fit through an opening.\\n    *   **Rough Tongue:** A cat\\'s tongue is covered in tiny spines called papillae, which help it strip meat from bones and groom its fur.\\n*   **Diet:** They are **obligate carnivores**, meaning their diet requires nutrients found only in animal flesh. They cannot survive on a vegetarian diet.\\n\\n---\\n\\n### 2. The Domestic Answer (The Companion)\\n\\nFor thousands of years, cats have lived alongside humans in a unique, mutually beneficial relationship.\\n\\n*   **Domestication:** Cats are believed to have been domesticated in the Near East around 7500 BC. They were attracted to human settlements by the rodents that fed on stored grain. Humans valued them for their pest-control abilities.\\n*   **Behavior and Personality:**\\n    *   **Independent:** Cats are known for their independence, but they also form deep, affectionate bonds with their human families.\\n    *   **Communicative:** They communicate through a wide range of vocalizations (meows, purrs, hisses, chirps) and complex body language (tail position, ear movement, slow blinks).\\n    *   **Playful:** They retain their hunting instincts through play, enjoying activities that involve stalking, chasing, and pouncing.\\n    *   **Clean:** Cats are fastidiously clean animals, spending a large portion of their day grooming themselves.\\n\\n---\\n\\n### 3. The Cultural Answer (The Symbol)\\n\\nCats hold a significant and often contradictory place in human culture and history.\\n\\n*   **Ancient Egypt:** They were revered and worshipped, most famously associated with the goddess **Bastet**. Harming a cat was a serious crime.\\n*   **Medieval Europe:** Their reputation soured, and they became associated with witchcraft and bad luck, particularly black cats.\\n*   **Modern Day:** Cats are one of the most popular pets in the world. They have also become **\"rulers of the internet,\"** dominating social media with countless photos, videos, and memes.\\n\\n---\\n\\n### Fun Facts about Cats:\\n\\n*   A group of cats is called a **clowder**.\\n*   Cats can make over 100 different sounds, whereas dogs can only make about 10.\\n*   They sleep for about 12-16 hours a day to conserve energy for hunting.\\n*   A cat\\'s purr is not fully understood, but it\\'s believed to be a form of communication and self-soothing. It may even have healing properties.\\n*   Cats cannot taste sweetness.\\n\\nIn essence, a cat is a fascinating paradox: a wild predator living in our homes, a symbol of both good and evil, and an independent creature that still craves our affection.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--fa30c068-f33a-4552-b7c9-9cbeafb71a31-0', usage_metadata={'input_tokens': 5, 'output_tokens': 878, 'total_tokens': 2404, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = llm.invoke(\"what is cat?\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f897426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate  #chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf303f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answers based on the questions.\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7413c136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=['Of course. As an AI Engineer, I use and follow LangSmith closely. It\\'s an essential tool in the modern LLM application development stack.\\n\\nLet\\'s break it down.\\n\\n### What is LangSmith?\\n\\nAt its core, **LangSmith is an all-in-one developer platform for building, debugging, monitoring, and evaluating LLM-powered applications.**\\n\\nThink of it as **Datadog, Sentry, or New Relic, but built specifically for the unique challenges of working with Large Language Models.** It was created by the same team that built LangChain, the popular open-source framework for developing LLM applications, and the two are seamlessly integrated.\\n\\n### The Core Problem LangSmith Solves\\n\\nBuilding a simple LLM prototype is easy. Building a reliable, production-ready LLM application is incredibly difficult. Why?\\n\\n1.  **Non-Determinism:** The same input can produce slightly different outputs, making traditional testing and debugging a nightmare.\\n2.  **The \"Black Box\" Problem:** When a complex chain or agent fails, it\\'s hard to know *why*. Was it a bad prompt? Did the LLM hallucinate? Did a tool fail to return the right data?\\n3.  **Prompt Engineering is Iterative:** Finding the perfect prompt requires endless trial and error. Managing and versioning these prompts is a mess.\\n4.  **Evaluation is Subjective:** How do you programmatically measure if one response is \"better\" than another? Standard metrics like accuracy don\\'t always apply.\\n5.  **Cost and Latency:** LLM calls can be slow and expensive. You need to know which parts of your application are causing bottlenecks or running up the bill.\\n\\nLangSmith is designed to solve exactly these problems by giving you full visibility into the entire lifecycle of your LLM application.\\n\\n---\\n\\n### Key Features and How They Help\\n\\nLangSmith\\'s functionality can be grouped into four main areas:\\n\\n#### 1. Tracing & Debugging\\n\\nThis is the most fundamental feature. Every time your LangChain (or other LLM-powered) application runs, LangSmith captures the entire execution path as a **\"trace.\"**\\n\\nA trace shows you a hierarchical view of every single step:\\n*   The initial input from the user.\\n*   Every LLM call, including the exact prompt sent and the raw response received.\\n*   Every tool or function call (e.g., a database query, an API call, a web search).\\n*   The inputs and outputs of each step in a chain or agent.\\n\\n**Why this is a game-changer:**\\n*   **Root Cause Analysis:** When an agent gives a weird answer, you can instantly see the exact \"thought\" process and pinpoint where it went wrong.\\n*   **Performance Tuning:** Each step in the trace is timestamped and includes token counts. You can immediately identify slow steps and high-cost LLM calls.\\n*   **Full Visibility:** No more `print()` statements. You have a persistent, detailed log of every run that you can inspect and share with your team.', '\\n\\n#### 2. Monitoring & Observability\\n\\nOnce your application is in production, LangSmith acts as your mission control. It provides dashboards to monitor key metrics in real-time:\\n*   **Latency:** Track P50, P90, P99 latencies for your application.\\n*   **Cost:** Monitor token usage and estimate costs across different models (GPT-4, Claude 3, etc.).\\n*   **Error Rates:** Get alerts on spikes in application failures.\\n*   **User Feedback:** You can programmatically attach user feedback (thumbs up/down, corrections) to specific traces, allowing you to identify and fix problematic interactions.\\n\\n#### 3. Testing & Evaluation\\n\\nThis is arguably LangSmith\\'s most powerful feature for ensuring quality. It provides a robust framework for testing your application\\'s performance.\\n\\nThe workflow looks like this:\\n\\n1.  **Create a Dataset:** You curate a dataset of example inputs and, optionally, their ideal \"ground truth\" outputs.\\n2.  **Run an Evaluation:** You run your LLM application against this dataset.\\n3.  **Apply Evaluators:** LangSmith runs \"evaluators\" on each output. These are themselves LLM-powered (or heuristic-based) checks that grade the responses based on criteria you define. Common evaluators include:\\n    *   **Correctness:** Is the answer factually correct compared to the ground truth?\\n    *   **Relevance:** Is the answer relevant to the user\\'s question?\\n    *   **Faithfulness / Groundedness:** Does the answer avoid hallucinating and stick to the provided context?\\n    *   **Toxicity/Harmfulness:** Does the response contain inappropriate content?\\n    *   **Custom Evaluators:** You can write your own evaluators for domain-specific logic (e.g., \"Did the response contain valid JSON?\").\\n\\nThe results are displayed in a clear matrix, allowing you to compare the performance of different prompts, models, or chain configurations side-by-side. This turns the subjective art of prompt engineering into a data-driven science.\\n\\n#### 4. The LangSmith Hub (Prompt Management)\\n\\nThe Hub is a central repository for managing, versioning, and sharing prompts. It\\'s like a \"GitHub for Prompts.\"\\n\\n*   **Discover:** Find high-quality, popular prompts for common tasks (e.g., RAG, Extraction, Agent \"thought\" prompts).\\n*   **Version Control:** Iterate on your prompts with full version history. You can easily roll back to a previous version if a change degrades performance.\\n*   **Collaborate:** Share prompts with your team, ensuring everyone is using the same, tested versions.\\n*   **Playground:** A built-in playground lets you test-drive prompts from the Hub instantly.\\n\\n### How Do You Use It?\\n\\nFor LangChain users, the setup is incredibly simple. You just need to set a few environment variables in your project:\\n\\n```bash\\nexport LANGCHAIN_TRACING_V2=\"true\"\\nexport LANGCHAIN_API_KEY=\"ls__...\"\\nexport LANGCHAIN_PROJECT=\"my-cool-project\" # Optional: to group runs\\n```\\n\\nWith these set, every LangChain run is automatically logged to your LangSmith project with zero code changes.\\n\\nFor non-LangChain users, LangSmith provides a lightweight Python SDK (`langsmith-sdk`) that allows you to manually create and log traces from any application, making it framework-agnostic.\\n\\n### Summary: Who is it for?\\n\\n*   **Individual AI Developers:** For rapidly debugging complex chains and agents.\\n*   **AI Engineering Teams:** For collaborating on prompts, evaluating model changes, and monitoring production applications.\\n*   **Product Managers & Stakeholders:** For understanding user interactions and overall application quality through monitoring and feedback dashboards.\\n\\nIn short, **LangSmith is the essential MLOps (or LLMOps) platform that bridges the gap between a simple LLM prototype and a robust, enterprise-grade AI application.** It provides the tooling necessary to manage the complexity and uncertainty inherent in building with language models.'], additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--5d1b9bbf-112e-4558-a540-5a9834083756-0', usage_metadata={'input_tokens': 24, 'output_tokens': 1516, 'total_tokens': 2757, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke({'input': 'can you tell me about langsmith?'})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "288a474a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7649266f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Of course. As an expert AI Engineer, I can give you a comprehensive overview of LangSmith.\\n\\nThink of it this way: If LangChain is the framework for *building* applications with Large Language Models (LLMs), then **LangSmith is the platform for *shipping* them.** It\\'s the essential toolkit for debugging, testing, evaluating, and monitoring your LLM applications, moving them from a cool prototype to a reliable, production-grade product.\\n\\nIn short, **LangSmith is to LLM applications what tools like Datadog, New Relic, or Sentry are to traditional software.** It\\'s an LLM-ops / MLOps platform specifically designed for the unique challenges of developing with language models.\\n\\n---\\n\\n### The Core Problem LangSmith Solves\\n\\nBuilding with LLMs is fundamentally different from traditional software engineering. You face unique challenges:\\n*   **The \"Black Box\" Problem:** When a complex chain of LLM calls, tools, and retrievers gives a bad output, it\\'s incredibly difficult to pinpoint *where* and *why* it failed.\\n*   **Non-Determinism:** The same input can produce slightly different outputs, making traditional testing difficult.\\n*   **Prompt Sensitivity:** A tiny change to a prompt can drastically alter performance. How do you systematically test and validate these changes?\\n*   **Cost & Latency:** LLM calls can be slow and expensive. You need to know which parts of your application are consuming the most tokens and time.\\n*   **Evaluation:** How do you objectively measure if one version of your application is \"better\" than another? Subjective \"it feels better\" doesn\\'t scale.\\n\\nLangSmith is built from the ground up to address these exact problems.\\n\\n### Key Features of LangSmith\\n\\nHereâ€™s a breakdown of its core components and what they do:\\n\\n#### 1. Tracing & Debugging\\n\\nThis is the cornerstone of LangSmith. Every time your LangChain application runs, LangSmith can capture the entire execution flow as a **\"trace.\"**\\n\\nA trace gives you a full, hierarchical view of the run:\\n*   **Full Visibility:** You can see every single step in your chainâ€”the LLM calls, the tool inputs/outputs, the documents retrieved from your vector database, etc.\\n*   **Input/Output at Each Step:** For each step, you can inspect the exact inputs it received and the exact outputs it produced. This is invaluable for debugging why a prompt is failing or why a tool isn\\'t being used correctly.\\n*   **Latency & Token Usage:** LangSmith automatically tracks how long each step took and how many tokens were consumed. This helps you immediately identify performance bottlenecks and cost centers.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt|llm|output_parser\n",
    "\n",
    "response = chain.invoke({\"input\":\"can you tell me about langsmith?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b040c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
