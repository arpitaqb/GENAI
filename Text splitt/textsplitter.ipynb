{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710572f6",
   "metadata": {},
   "source": [
    "### Text splitting from document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401b1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file \n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bcaf9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Received February 15, 2017, accepted February 26, 2017, date of publication March 3, 2017, date of current version March 28, 2017.\\nDigital Object Identifier 10.1 109/ACCESS.2017.2676158\\nA Novel Text Structure Feature Extractor for\\nChinese Scene Text Detection and Recognition\\nXIAOHANG REN1, YI ZHOU1, ZHENG HUANG2, JUN SUN1, (Member, IEEE),\\nXIAOKANG YANG1, (Senior Member, IEEE), AND KAI CHEN1, (Member, IEEE)\\n1Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai 200240, China\\n2Institute of Information Security and Engineering, Shanghai Jiao Tong University, Shanghai 200240, China\\nCorresponding author: Y . Zhou (zy_21th@sjtu.edu.cn)\\nThis work was supported in part by the National Key Research and Development Program of China under Grant 2016YFB1001003, in part\\nby the National Natural Science Foundation of China under Grant 61521062 and Grant 61527804, in part by the Shanghai Science and\\nTechnology Committees of Scientiﬁc Research Project under Grant 14XD1402100 and Grant 15JC1401700.\\nABSTRACT Scene text information extraction plays an important role in many computer vision applications.\\nMost features in existing text extraction algorithms are only applicable to one text extraction stage (text\\ndetection or recognition), which signiﬁcantly weakens the consistency in an end-to-end system, especially\\nfor the complex Chinese texts. To tackle this challenging problem, we propose a novel text structure feature\\nextractor based on a text structure component detector (TSCD) layer and residual network for Chinese texts.\\nInspired by the three-layer Chinese text cognition model of a human, we combine the TSCD layer and\\nthe residual network to extract features suitable for both text extraction stages. The specialized modeling\\nfor Chinese characters in the TSCD layer simulates the key structure component cognition layer in the\\npsychological model. And the residual mechanism in the residual network simulates the key bidirectional\\nconnection among the layers in the psychological model. Through the organic combination of the TSCD\\nlayer and the residual network, the extracted features are applicable to both text detection and recognition,\\nas humans do. In evaluation, both text detection and recognition models based on our proposed text structure\\nfeature extractor achieve great improvements over baseline CNN models. And an end-to-end Chinese text\\ninformation extraction system is experimentally designed and evaluated, showing the advantage of the\\nproposed feature extractor as a uniﬁed feature extractor.\\nINDEX TERMS Text structure feature, Chinese text, deep learning, residual network, uniﬁed model.\\nI. INTRODUCTION\\nText, an abstract presentation of artiﬁcial information, is scat-\\ntered throughout the human society in this succinct present-\\ning age. As portable digital recording devices are rapidly in\\nfashion among ordinary people, natural images and videos\\ncontents proliferate in image and video sharing websites, e.g.\\nYouTube and Flickr. By extracting text information, which\\ncarries high-level semantics, natural media contents can be\\neffectively understood and used. It is crucial for a wide range\\nof applications such as image classiﬁcation, scene recogni-\\ntion and automatic navigation in urban environments.\\nWhile text recognition in scanned documents has been well\\nstudied and successfully deployed in real-world applications,\\nthe detection and recognition of texts in uncontrolled environ-\\nments still remains an open issue. Generally, text information\\nextraction can be divided into two stages: text detection and\\ntext recognition. Traditionally, most works place the two\\ncomponents in consecutive stages, where text detection algo-\\nrithms commit to tackle the challenges of the variations of text\\nfont, size and style, complex backgrounds, noise, and uncon-\\nﬁrmed lighting conditions (like using ﬂash lamps) [1]–[8]\\nand text recognition algorithms of the huge variations of\\ntext layouts, orientations, geometric distortions and partial\\nocclusions [9]–[13]. These challenges have spawned various\\nfeature designs to meet the needs of one stage. Recently,\\na variety of end-to-end text information extraction algo-\\nrithms [14]–[18] are proposed that unify the features for both\\nstages to remove the error propagation between them, leading\\nto signiﬁcant improvements.\\nHowever, existing works still have certain limitations.\\nIn particular, most of them only focus on English texts,\\nwhich are relatively easy to deﬁne and recognize due to the\\nsimplicity in strokes and structures. In this age of globaliza-\\ntion, the recognition of multilingual texts attracts increasing\\nVOLUME 5, 2017\\n2169-3536 \\n 2017 IEEE. Translations and content mining are permitted for academic research only.\\nPersonal use is also permitted, but republication/redistribution requires IEEE permission.\\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\n3193'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\ninterests. Among them, the logographic text is the most spe-\\ncial text type, which presents both pronunciation and meaning\\nin its appearance. As a typical logographic text, Chinese\\nreveals signiﬁcant different properties than English, which is\\na typical Latin-based text and has been well studied, from the\\nfollowing aspects:\\n1. Number of strokes. Most Chinese characters contain\\nmore than ﬁve strokes, while the most complex English\\ncharacter only has four.\\n2. Type of strokes. There are more than 30 different types\\nof Chinese strokes, while only 10 different types of\\nstrokes exist in English.\\n3. Style of characters. It is widely acknowledged that most\\nChinese characters are picto-phonetic, while English\\nuses abstract characters.\\n4. Intra-character structures. Chinese characters are more\\ncomplicated than English, where certain structures may\\nexist within a character.\\nDue to the aforementioned differences between Chinese\\nand English texts, the study of Chinese text recognition is\\nof much value in both theoretical and practical perspec-\\ntives. Unfortunately, current state-of-the-art text informa-\\ntion extraction algorithms for English texts cannot be easily\\ndeployed to Chinese. Because of the complexity of Chinese\\ntexts including the strokes and intro-character structures, it is\\nextremely difﬁcult for an algorithm to combine text detection\\nand recognition in a uniﬁed framework.\\nIn this paper, a Chinese text structure feature extractor\\nis designed, trained and applied to both text detection and\\nrecognition, making a number of key contributions.\\nOur main contribution is a novel Chinese text structure\\nfeature extractor. Motivated by the three-layer Chinese text\\ncognition model of human in psychology, we combine the\\ntext structure component detector (TSCD) layer and the resid-\\nual network to simulate the two key mechanisms in the three-\\nlayer model. The TSCD layer has specialized modeling for\\nChinese structure components that are the bridge between\\nstrokes and characters and play key role in the three-layer\\nmodel. The residual network has its unique residual mech-\\nanism that is an effective bidirectional information transmis-\\nsion between the upper and lower layers. It is highly similar\\nto the key bidirectional connection in the three-layer model.\\nBy reconstructing the components in the TSCD layer into\\na TSCD block (shown in Fig. 1) refer to the design idea\\nof residual network, the organic combination is established\\nand the three-layer Chinese text cognition model is well\\nsimulated. Therefore, the extracted features are applicable to\\nboth stages in Chinese text information extraction.\\nOur second contribution is unifying Chinese structure fea-\\nture extractor in both text detection and recognition with\\nsharing trained parameters. In deep learning models, both text\\ndetection and recognition are regarded as classiﬁcation tasks.\\nIn our detection and recognition models, the feature extractor\\nis trained in the recognition model, which focuses more on\\nthe structure features in Chinese characters. The structure\\nfeatures are unique and rarely seen in background regions,\\nFIGURE 1. The TSCD block.\\nthus they are applicable to distinguish text and background\\nregions without ﬁne-turning the extractor parameters.\\nOur third contribution is a synthetic data engine. As the size\\nof public available Chinese scene text datasets is not enough\\nto train deep models, it is essential to expand the training\\ndata with artiﬁcial samples. The synthetic data engine is\\ncomposed of three stages, each of them simulates one scene\\ntext characteristic. Thus, the generated samples can partially\\nsubstitute the scene text images and become an essential\\ndata component in training the Chinese text structure feature\\nextractor.\\nThe rest of the document is organized as follows.\\nIn Section II, we introduce the related works of our work.\\nIn Section III, we describe the proposed Chinese text structure\\nfeature extractor. In Section IV , we present the experimental\\nevaluation setting up, results and discussions. The paper is\\nconcluded in Section V .\\nII. RELATED WORKS\\nGenerally, text information extraction is divided into two\\nstages: text detection and text recognition. As text recogni-\\ntion in scanned documents is well studied and many OCR\\nsystems have achieved quite good performance, most existing\\nresearches focus on the text detection stage.\\nText detection approaches concern how to discover and\\nlocate the regions containing texts from scene images [19].\\nTraditionally, there are two major categories in text detec-\\ntion algorithms: region-based and texture-based. The sym-\\nbolic component of region-based algorithms is the image\\nregion extraction that limits the feature analyzation on image\\npatches. The features in these algorithms are always unique\\nin scene text regions. Sliding window based algorithms\\nand connected component (CC)-based algorithms are the\\nmain region-based algorithm types. Sliding window based\\n3194 VOLUME 5, 2017'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nalgorithms extract numerous overlapping image rectangles\\nfor feature extraction [2], [14], [20]. CC-based algorithms\\nextract connected image regions with uncertain shapes\\nand use a set of rules to identify scene text regions [3],\\n[4], [21]–[23]. The symbolic component of texture-based\\nalgorithms is the global texture features in the entire scene\\nimage. By analyzing the distribution of the global texture\\nfeatures, text regions are extracted from the scene image\\nand connected into text lines. Machine learning methods are\\nthe most popular tools in texture-based algorithms, which\\nhave strong capability to extract text regions with the global\\ntexture features [1], [5], [7]. As scene texts can be in various\\norientations, multi-orientation approaches provide a practical\\nsolution to scene text detection. Component analyzing is the\\ncore in recent multi-orientation approaches, including com-\\nponent aggregating, clustering and linking [24]–[26].\\nText recognition aims to extract text content information\\nfrom the cropped text images. As different language texts\\nare in great disparities, most existing scene text recognition\\nalgorithms focus solely on recognizing one language texts.\\nAmong them, English text recognition is widely studied\\nand can be classiﬁed into two major categories: character\\nbased recognition and word based recognition [27]. Character\\nbased recognition recognizes the characters in the image by\\na character classiﬁer and combine the recognize results into\\nwords. As English character types are very limited, most\\ncommon classiﬁers are competent to the task with appropriate\\nfeatures [10], [12], [28]. Word based recognition directly\\nrecognizes the word in the text image. As there are numerous\\nEnglish words, common classiﬁers or models are not com-\\npetent to recognize words directly. So in some algorithms,\\nthe efﬁcient components of character based recognition are\\nstill adopted [9], [13], [29].\\nThe strategy of performing text detection and text recog-\\nnition in separate and independent stages may have certain\\nproblems. As the goal of text detection and the requirement\\nof text recognition occur discrepancies in many aspects, it is\\nargued that additional challenges may turn up when exploit-\\ning the text detection results for text recognition. Therefore,\\na variety of end-to-end text information extraction algo-\\nrithms [14], [17], [18], [30] are proposed to combine both text\\ndetection and recognition stages by a uniﬁed framework. The\\nkey mechanism in the uniﬁed framework is uniﬁed features\\nwhich are applicable to both text detection and recognition.\\nDue to the higher complexity of Chinese texts than English\\ntexts, the design of uniﬁed features is much more difﬁcult.\\nIn our previous work [31], a TSCD layer is designed to extract\\nChinese structure features in CNN based model. The structure\\nfeatures are essential in Chinese characters, thus they are\\napplicable for both Chinese text detection and recognition.\\nRecently residual network proposed by He et al. [32] is\\nunder the spotlight in deep learning researchers. They design\\nseveral convolutional blocks with residual compute and feed-\\nback routes, which enable the top parameters in a very deep\\nmodel learned effectively by current training method. Ben-\\neﬁt from the convolutional blocks, very deep convolutional\\nneuronal networks, e.g. 152 layers and 1001 layers, can be\\neasily designed. And they have achieved huge improvements\\nin classiﬁcation, detection and segmentation tasks. Further-\\nmore, the convolutional blocks also quicken the bidirec-\\ntional information transmission route between layers. Thus\\nthe design of convolutional block is instructive for other tasks\\nthat require bidirectional information transmission.\\nSynthetic data engines for artiﬁcial text image generation\\nachieve great success in deep learning based scene English\\ntext information extraction as the size of public available\\nEnglish text dataset is not enough for training a deep model.\\nIn [16], numerous simple synthetic text images are gener-\\nated and used to train text detection and recognition models.\\nA complete synthetic data engine is designed in [33], which\\nsimulates English word regions in scene images. In [34], a fast\\nand scalable synthetic data engine that naturally blends text in\\nexisting natural scenes is proposed to generate scene images\\nwith text regions. As there are fewer public available Chinese\\ntext datasets and Chinese texts are more complex, it is in need\\nof a synthetic data engine for Chinese texts.\\nIII. CHINESE TEXT STRUCTURE FEATURE EXTRACTOR\\nThe design of the proposed Chinese text structure feature\\nextractor is motivated by the three-layer Chinese text cog-\\nnition model of human, which is described in Section III.A.\\nThe text structure component detector (TSCD) layer and\\nthe residual network simulate the key mechanisms in the\\nmodel: structure component layer, which is the bridge in\\nthe model, and the bidirectional information transmission\\nbetween the upper and lower layers, respectively. Based on\\ntheir design ideas, a TSCD block is constructed with various\\nstructure feature extractors in residual information transmis-\\nsion route, which is described in Section III.B. Replacing a\\nset of convolutional blocks in a residual network model with\\nTSCD blocks, Chinese text structure feature can be extracted\\naccurately to meet the requirements of both text detection\\nand recognition. In order to enhance the unity of the text\\ndetection and recognition models, the Chinese text structure\\nfeature is applied as uniﬁed features in the two models, whose\\nsharing structure is described in Section III.C. The network\\nparameters of the uniﬁed feature extractor are learned solely\\nin recognition model with a training set consisting of scene\\nand artiﬁcial samples. A considerable number of samples\\nin the training set are artiﬁcial text images generated by a\\nsynthetic data engine proposed in Section III.D. The engine\\ncontains three generation stages, which cover every essential\\nprocess from putting a text in the scene to taking a text\\nimage, to ensure the effectiveness of the generated samples\\nin training.\\nA. CHINESE TEXT COGNITION MODEL OF HUMAN\\nWhen human extract text information from vision, text detec-\\ntion and recognition stages are not independent and sequen-\\ntial. They work at the same time and complement each other:\\ndetection process assists recognition, and recognition pro-\\ncessing assists detection at the meanwhile. Thus we believe\\nVOLUME 5, 2017 3195'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nthat the extracted features in human cognition system for text\\nare applicable to both detection and recognition. By analyzing\\nand simulating the Chinese text cognition model of human,\\nwe can design a feature extractor for both text detection and\\nrecognition.\\nBased on the Chinese text recognition multi-layer activa-\\ntion model proposed by Taft and Zhu [35], Shen et al.[36]\\nproposed a three-layer Chinese text cognition model of\\nhuman, which has been widely accepted by the psychology\\ncommunity. In the cognition model (shown in Fig. 2), the Chi-\\nnese text cognitive process can be divided into three layers:\\ntext layer in the top, text structure component layer in the mid-\\ndle and stroke layer in the bottom. Psychological experiments\\nshow that the information transmission between two adjoin-\\ning layers is bidirectional, which has top-down and bottom-up\\ntwo transmission directions (represented by the arrow direc-\\ntions in Fig. 2). Furthermore, the bidirectional information\\ntransmission can be excitatory and inhibitory (represented by\\nthe solid and dashed arrows in Fig. 2).\\nFIGURE 2. The cognition model.\\nIt can be seen from the model, as the middle layer of the\\nthree-layer model, the text structure component has direct\\nconnection and interaction to upper text and lower stroke.\\nSo the structure component is the bridge and key structure\\nin the Chinese text cognition model of human. And the\\nbidirectional connections strengthen the speed and ﬂexibility\\nof information transmission. So the bidirectional information\\ntransmission is the key transmission in the Chinese text cog-\\nnition model of human. Therefore, the key to simulate the\\nhuman text cognition feature lies in the simulation of the two\\nkey mechanisms: structure and information transmission.\\nB. TEXT STRUCTURE COMPONENT\\nDETECTOR (TSCD) BLOCK\\nChinese character is a kind of pictographs, which contains a\\nlarge number of radicals and structures. Compared with other\\nnon-Latin language scripts, i.e. Japanese, Koran and Arabic\\nscripts, Chinese script has richer character structures, which\\nmakes the structure features more important in Chinese script\\nthan other language scripts. There are many structure types in\\nmodern Chinese characters, which contain four basic types:\\nleft-right structure, top-bottom structure, inner-outer struc-\\nture and single character [37] and their sub-types such as left-\\nmiddle-right, top-left-right and so on. Each Chinese character\\nis comprised of several structure components according to its\\nstructure type. Following the utility function proposed in [38],\\nChinese character structure components are extracted from\\nthe most commonly-used 1290 Chinese characters based on\\ntheir basic structure types. Although Chinese characters con-\\ntain a wide variety of the structure components in appearance,\\ntheir aspect ratio types are limited by their structures. Thus\\nthe structure components are able to be classiﬁed into a few\\ntypes according to their aspect ratio types. An example of the\\nstatistical result of top-bottom structure is shown in Fig. 3,\\nin which structure components extracted from top-bottom\\nstructure characters are classiﬁed into 10 types. It can be\\nseen from the statistical result that the top-bottom structure\\ncharacters contain three main structure component types(3:1,\\n3:2 and 2:1) and three secondary types(1:2, 1:1 and 2:3).\\nAccording to the aspect ratio types of Chinese char-\\nacter structure components, several text structure compo-\\nnent detectors are designed in convolutional neural network.\\nAccording to its target character structure component type,\\nthe aspect ratio of the convolutional window in a text structure\\ncomponent detector is set as the same. The longer edge of the\\nconvolutional window is ﬁxed. Convolutional window deter-\\nmines what kind of feature the convolutional feature extrac-\\ntion is adapt in. For example, a 2:1 convolutional window is\\nmuch more sensitive to structure components with 2:1 aspect\\nratio than other convolutional windows and is insensitive\\nto structure components with other aspect ratios. Thus it\\nwill focus on extracting accurate features from 2:1 structure\\ncomponents after effective training. Those text structure com-\\nponent detectors with various convolutional window sizes\\nextract structure component features in parallel and form a\\nTSCD layer. Thus, the TSCD layer well simulates the struc-\\nture of structure component layer in the three-layer Chinese\\ntext cognition model of human. More details of TSCD layer\\nare proposed in our previous work [39].\\nIn residual network [32], the basic units are residual con-\\nvolutional blocks. A residual convolutional block contains\\nseveral convolutional layers, batch normalization layers and a\\nshortcut route. The input features are separated to the shortcut\\nroute and the layers, whose functionality is to compute the\\nresidual of the feature. And the input features and the residual\\nfeatures are merged in the end. The information transmis-\\nsion in a residual convolutional block is quick and ﬂexible\\ndue to the shortcut route and the residual computing and\\nmerging. And because of forward and backward propaga-\\ntions in deep network models, the information transmission\\nis bidirectional. Thus, the residual network well simulates\\nthe information transmission in the three-layer Chinese text\\ncognition model of human.\\nThe TSCD layer and the residual network both simulate\\none of the two key mechanisms in the three-layer Chinese\\ntext cognition model of human. By effectively combining\\n3196 VOLUME 5, 2017'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nFIGURE 3. The statistical result of top-bottom structure characters.\\nFIGURE 4. The statistical result of Chinese character structure component aspect ratio types.\\nthem, the three-layer model can be well simulated and image\\nfeatures applicable to both Chinese text detection and recog-\\nnition can be extracted accurately. However, though they are\\nboth based on CNN, their design directions are completely\\ndifferent. Simply connecting them into a CNN model cannot\\nequip the model with the two key mechanisms. Thus, we com-\\nbine them in design idea, to construct a convolutional struc-\\nture that not only has a feature extraction capacity of various\\nstructure component types as the TSCD layer, but also has\\na quick and ﬂexible information transmission as the residual\\nnetwork. The design idea is to reconstruct the convolutional\\nblock of residual network with more convolutional window\\ntypes and place their residual computations in parallel, which\\nextracts various Chinese text structure features exclusively\\nand keeps their independence for accuracy.\\nBased on the above design ideas, we propose a TSCD block\\nto completely combine the TSCD layer and residual network.\\nThe TSCD block is a special layer combination in CNN\\nmodel, whose structure overview is shown in Fig. 1. In the\\nTSCD block, the various components in the TSCD layer are\\nbroken up and reconstructed in parallel with batch normaliza-\\ntion and activation to compute their residual independently.\\nThe activation function is Rectiﬁed Linear Units (ReLu),\\nwhich simulates the sparse activation of human neurons.\\nUsing linear activation function also alleviates the Vanish-\\ning Gradient Problem in training. As there is no correlation\\namong the structure component types, the residuals of each\\nfeature types should be computed individually to keep their\\nindependence, which also reduces the computational com-\\nplexity. After the residual computations are completed, they\\nare connected into undivided features, which has the same\\nsize as the input features, using concat function. In this way,\\nthe inﬂuences among different component types in residual\\ncomputations are avoided. Finally, the input features and the\\nresidual features are merged into the output features as they\\ndo in a convolutional block in residual network. It should be\\nnoted that if the input features are stroke features extracted by\\na convolutional block rather than structure features extracted\\nby a TSCD block, there will be a TSCD layer in the shortcut\\nroute to extract structure features. The feature type order and\\nsizes in this TSCD layer are exactly the same as those in the\\nresidual computing route of the TSCD block, which ensures\\nthe input features and the residual structure features are\\ncorrectly matched according to their structure types before\\nmerging.\\nThrough the statistical result (as shown in Fig. 4), the\\nChinese text structure component types to be detected and\\nextracted in the TSCD block can be determined. Those struc-\\nture component types are extracted from the most commonly-\\nused 1290 Chinese characters and classiﬁed by their aspect\\nratios. More than 99% structure components can be classiﬁed\\nin to the 11 types shown in the statistical result. Among them,\\nthe least three structure component types accounted for less\\nthan 1% of the total. Thus, we choose the most common\\neight structure component types to be detected and extracted\\nfeatures in the TSCD block. To balance the feature inﬂuences\\nof different structure component types in the extracted fea-\\ntures, the feature sizes have the same proportions as their cor-\\nresponding structure component types. With such a design,\\nthe quantitative distribution of text structure features matches\\nthat of natural Chinese text structure component types, which\\ngrantees the most Chinese character structure components\\ncan be detected and extracted features in the TSCD block.\\nOur proposed Chinese text structure feature extractor is\\nconstructed by two convolutional block sets and one TSCD\\nblock set. The two convolutional block sets are applied to\\nVOLUME 5, 2017 3197'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nextract accurate stroke features from input image. The TSCD\\nblock set is applied to extract speciﬁed Chinese text structure\\nfeatures from the stroke features with bidirectional informa-\\ntion transmission. In this way the two lower layers in three-\\nlayer Chinese text cognition model of human are simulated\\nin a deep learning model based on their two key mechanisms:\\nstructure and information transmission. Thus, the extract fea-\\ntures are applicable to both text detection and recognition.\\nC. UNIFIED FEATURES IN TEXT DETECTION AND\\nRECOGNITION MODELS\\nIn current deep learning model design, it is difﬁcult and costly\\nto get detection and recognition results from an output layer.\\nSo we do not unify the Chinese text detection and recognition\\nmodels from top to bottom, but we input the features into\\ndifferent fully connected and output layers after the uniﬁed\\nfeature extraction.\\nFIGURE 5. The unified Chinese structure feature extractor in text\\ndetection and recognition models.\\nFig. 5 shows the overview of the uniﬁed Chinese\\nstructure feature extractor in text detection and recognition\\nmodels. In the top, the proposed Chinese text structure feature\\nextractor is applied to extract the uniﬁed feature from input\\nimage patches. Then the features are ﬂattened and inputted to\\ntwo different analyzing structures. One is the text detection\\nstructure, which contains a fully connected layer to analyze\\nthe features for text detection and a binary softmax classiﬁer\\nto output the detection result of whether the input image\\npatch is a text patch or a background patch. The other is the\\ntext recognition structure, which contains a fully connected\\nlayer to analyze the features for text recognition and a multi-\\nclass softmax classiﬁer to output the recognition result of\\nwhich Chinese character the input image patch represents.\\nIn the view of text information system, these structures can\\nbe regarded as sharing feature extraction in text detection and\\nrecognition models.\\nAs text recognition requires more accurate and unique text\\nstructure features than text detection, the shared Chinese text\\nstructure feature extractor is trained in the text recognition\\nmodel with text images. Beneﬁt from the unique text structure\\nfeatures in Chinese, the extracted features for text recognition\\nwork well in text detection model. So the feature extractor\\ntrained for text recognition has not to be ﬁne-tuned for text\\ndetection, which simpliﬁes the training process and reduces\\nthe computational complexity of detection and recognition.\\nDue to the sharing mechanism, the model unity is signiﬁ-\\ncantly improved than that in [31], which has uniﬁed structure\\nof feature extraction.\\nD. SYNTHETIC DATA ENGINE\\nChinese text is more complex than English text in stroke,\\nstyle and structure, which imposes higher requirements on the\\nquantity and quality of training images to train credible deep\\nmodels. However, the size of public available scene Chinese\\nscene text datasets is much smaller than English text and far\\nfrom enough to satisfy the requirement. Inspired by the suc-\\ncessful use of artiﬁcial text images in English text information\\nextraction deep model [18], we design a synthetic data engine\\nto generate high quality artiﬁcial Chinese character images.\\nConsidering the high similarity of Chinese fonts, charac-\\nters are generated by several representative Chinese fonts\\nrather than all the available fonts. Meanwhile, as Chinese\\ncharacters are all square shaped, we randomly assign the\\ncharacter size in generation. Our synthetic data engine con-\\nsists of three stages: character image generation, image trans-\\nformation, camera inﬂuence generation. Their details are as\\nfollows:\\n1. Character image generation: The character image are\\ngenerated as we usually do when printing characters\\nin nature. There are three steps in this stage. Firstly,\\ncharacters are generated by Chinese fonts. As many\\nChinese fonts are highly similar, we collect 32 Chinese\\nfonts, which includes three basic fonts Kai, Song and\\nHei, 24 derived fonts such as FangSong, XiHei and ﬁve\\ncommon special fonts, to generate characters. (When\\ngenerating characters, 30% of them are basic fonts,\\n60% are derived fonts and 10% are special fonts.)\\nSecondly, the character color and background color are\\nrandomly selected form the most common 27 colors in\\nscene text images. Finally, borders with random thick-\\nness and color are added randomly to the characters.\\n2. Image transformation: To simulate the characters in\\nscene environment, some natural transformations are\\nadded to the generated character images. There are\\nthree steps in this stage. Firstly, shadows with ran-\\ndom orientation, thickness and intention are added\\nrandomly to the characters, which simulates nature\\nlight. Secondly, full-projective transformation method\\nare used the transform the image with random\\nparameters, which simulates different viewpoints of\\nobserver. Finally, scene background image patches are\\nblended to the characters with random blend inten-\\nsity and blend mode, which simulates the reﬂection in\\nnature.\\n3. Camera inﬂuence generation: In this stage, some types\\nof image noise, which is common in taking pictures\\nfrom natural, are added to the image. In our engine,\\n3198 VOLUME 5, 2017'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nwe add Gaussian noise and Gaussian blur to the char-\\nacter image with random intensity.\\nThe representative Chinese character images generated by\\nour synthetic data engine with random parameters perfectly\\nsimulates the text regions in scene images. And they become\\nessential components in the training set to train our proposed\\nChinese text structure feature extractor.\\nIV. EXPERIMENTS\\nIn this section the proposed Chinese text structure feature\\nextractor is evaluated with scene text detection and recog-\\nnition models. The feature extractor is ﬁrst evaluated with\\ntext recognition model as it is trained in the model. Then\\nit is evaluated with text detection model. Finally the detec-\\ntion and recognition models are connected and the proposed\\nfeature extractor is evaluated as a uniﬁed feature extractor.\\nThree multilingual text datasets [6], [40], [41] are used in the\\nevaluations, one of them is used in all the evaluations and\\nthe other two are used in either text detection or recognition\\nevaluation.\\nIn Section IV .A, the datasets used in evaluations are\\npresented. Then the details of training samples, deep\\nmodels, training methods are introduced in section IV .B.\\nIn section IV .C, the text recognition evaluation results are\\npresented and discussed. The text detection evaluation results\\nare presented and discussed in section IV .D. The evaluation\\nresults of connected models are presented and discussed\\nin section IV .E.\\nA. DATASETS\\nThree multilingual datasets are used to evaluate our pro-\\nposed Chinese text structure in different models. Among\\nthem, Ren’s dataset [6] is used in all the evaluations. Zhou’s\\ndataset [40] is used in the evaluation of text recognition and\\nPan’s dataset [41] is used in the evaluation of text detection.\\nA summary of the datasets is shown in Table I.\\nTABLE 1. A description of the various datasets evaluated on.\\nRen’s dataset is a multilingual scene text detection and\\nrecognition dataset proposed by us in 2015. There are\\n194 images in the training set and 200 images in the testing\\nset. All the training images and half of the testing images are\\ntaken from open ﬁelds with different weathers and indoor\\nwith different lights from various objects by a camera. The\\nother half of the testing images are taken by various photogra-\\nphers on the Internet. All of them are scene images and taken\\nby various shutterbugs in natural. And all the text regions\\nhave language labels to improve the evaluation accuracy of\\nparticular language types.\\nZhou’s dataset is a multilingual text detection and recog-\\nnition dataset proposed in 2015. The training set contains\\n483 images and the testing set contains 484 images. It is\\nnoted that not all the images in the dataset are scene images.\\nA considerable amount of images are artiﬁcial text images\\nincluding Internet advertisements and screenshots.\\nPan’s dataset is a multilingual scene text detection dataset\\nproposed in 2011. It is popular in multilingual text detection\\nalgorithm evaluations. There are 248 images in the training\\nset and 239 images in the testing set. It is noted that the\\ntext regions in the dataset do not have language labels. Thus,\\nit can only evaluate the general multilingual text detection\\nperformance with all language types.\\nCompared with other image detection and recognition\\ndatasets, scene text detection and recognition datasets are\\nsmaller, in which training set and testing set generally contain\\nonly a few hundred images. However, because many text\\nand text regions are contained in a scene image, hundreds of\\ntraining and test images are sufﬁcient to comprehensively and\\neffectively evaluate text detection and recognition algorithms.\\nCompared with the most widely used text detection dataset\\nICDAR 2011 [42], which contain 229 train images and\\n255 test images, the above datasets contain similar or more\\nscene text images. Note that although Ren’s dataset contains\\na bit less (194, 200) images, it is expected that using our\\ndataset can be more effective for evaluation of our proposed\\nChinese text detection and recognition models as the labels\\nin our dataset are more comprehensive.\\nFIGURE 6. The training samples. (a) The examples of the artificial image\\npart. (b) The examples of the natural image part.\\nB. TRAINING DETAILS\\n1) TRAINING SAMPLES\\nThe training samples to train the proposed Chinese text\\nstructure feature extractor in the text recognition model are\\ncomposed of two parts (examples are shown on Fig. 6). One\\npart is the artiﬁcial Chinese character scene images gener-\\nated by our proposed synthetic data engine (Section III.E).\\nThere are 96,000 artiﬁcial character images generated with\\n1500 Chinese characters(64 images each) which are com-\\nposed of the most commonly used characters and the char-\\nacters in the multilingual datasets. The other part is the\\nscene character images extracted from the training sets of\\nthe multilingual datasets. The models evaluated in different\\ndatasets are trained with different scene training sets. There\\nare 6000 character images extracted in Ren’s dataset and\\n7000 character images extracted in Zhou’s dataset.\\nThe training samples to train the text detection model with\\nthe proposed Chinese text structure feature extractor are the\\nimage patches extracted by the multilingual datasets using\\nVOLUME 5, 2017 3199'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nmulti-scale sliding window method. In Ren’s dataset, there\\nare approximately 25,000 training samples being extracted\\nand selected. The number of simple text samples, complex\\ntext samples and background samples is 3000, 6000 and\\n16000, respectively. The text region percentage in a simple\\ntext sample, complex text sample and background sample\\nis over 80%, 25% to 66% and less than 10%, respectively.\\nIn Pan’s dataset, there are approximately 35,000 training sam-\\nples being extracted and selected, in which 5000 are simple\\ntext samples, 9000 are complex text samples and 21,000 are\\nbackground samples.\\n2) DEEP MODELS\\nThe proposed Chinese text feature extractor is mainly com-\\nposed of one convolutional layer, two convolutional block\\nsets and one TSCD block set in sequential. Each block set\\ncontains three blocks. The convolutional layer has 16 ﬁlters\\nwith 3 ×3 window size. The convolutional blocks in the ﬁrst\\nset all have 32 ﬁlters and 3 ×3 window size, while those\\nin the second set all have 64 ﬁlters and the same window\\nsize. The output features of each convolutional block set are\\ndown-sampled by 2 ×2 max-pooling. Each layer in the TSCD\\nblocks has 128 ﬁlters in total, which are distributed to eight\\ntext structure component types according to their proportions.\\nThe output features of the TSCD block set are down-sampled\\nby average-pooling with pool-size of 4 ×4 and output as\\nthe extracted features of the proposed Chinese text feature\\nextractor.\\nThe proposed Chinese scene text recognition model is\\ncomposed of the Chinese text feature extractor, a fully con-\\nnected layer and a softmax classiﬁer in sequential. The fully\\nconnected layer has 2048 units. The softmax classiﬁer has\\n1500 classiﬁcation categories corresponding to the 1500 Chi-\\nnese characters.\\nThe proposed Chinese scene text detection model is also\\ncomposed of the Chinese text feature extractor, a fully con-\\nnected layer and a softmax classiﬁer in sequential. The fully\\nconnected layer has 512 units. The softmax classiﬁer has\\ntwo classiﬁcation categories corresponding to text region and\\nbackground region.\\n3) TRAINING STEPS\\nThe proposed Chinese text feature extractor is trained in the\\nscene text recognition model. And the fully connected layer\\nand the softmax classiﬁer in the recognition model in the\\nrecognition model are trained at the same time. Firstly the\\nrecognition model is trained by using all the character images\\nin the artiﬁcial part. Then it is further trained by using all the\\nscene character images and 7500 artiﬁcial character images,\\nwhich are randomly selected from the artiﬁcial part and each\\ncharacter has ﬁve samples. The two-step training balances the\\nlarge amount of artiﬁcial character samples and the smaller\\namount but more realistic scene character samples to ensure\\nthe recognition accuracy.\\nThen the trained Chinese text feature extractor is applied\\nin the scene text detection model to train the fully connected\\nlayer and the softmax classiﬁer of in the model. During\\nthe model training, the parameters in feature extractor are\\nﬁxed and only the fully connected parameters are updated.\\nIn this way, the features extracted by our proposed Chinese\\ntext feature extractor can be used in both text detection and\\nrecognition models as a uniﬁed feature.\\nC. CHINESE SCENE TEXT RECOGNITION\\nThe proposed Chinese text structure feature extractor is com-\\npletely trained in text recognition model. So the performance\\nof the Chinese scene text recognition model is the key evalua-\\ntion of the proposed feature extractor. And the trained feature\\nextractor will be directly applied to the Chinese scene text\\ndetection model, the quality and accuracy of its extracted fea-\\ntures almost determine the performance of the text detection\\nmodel. Therefore, we ﬁrstly conduct several experiments to\\nevaluate effectiveness of the proposed feature extractor in text\\nrecognition.\\nThe synthetic data engine we designed in Section III.D\\ngenerates numerous high quality artiﬁcial Chinese character\\nimages for training the text recognition model. Its effective-\\nness is evaluated by training the text recognition model with\\ndifferent training set formations.\\nThe Chinese text recognition model with the proposed\\nChinese text structure feature extractor is evaluated on two\\ntext recognition datasets. One of them is an all natural text\\nimage dataset [6] in which all the text images are taken from\\nnatural. The other dataset [40] is a partly natural text image\\ndataset which contains a number of artiﬁcial text images like\\nInternet ADs. Their details are shown in Section IV .A.\\nTABLE 2. Chinese text recognition results.\\nTable II summarizes the Chinese scene text recognition\\nevaluation results with different training set formations and\\ndeep models. The ABBYY , which is a well-known OCR\\nsystem, is evaluated as baseline. ‘‘S,’’ ‘‘A’’ and ‘‘A+S’’ repre-\\nsent different training set formations. ‘‘S’’ represents only the\\nscene character images extracted from the respective training\\nsets are used in training and ‘‘A’’ represents only the artiﬁcial\\ncharacter images generated by the synthetic data engine are\\nused. ‘‘A+S’’ represents the artiﬁcial and scene character\\nimages are all used to train the deep model. The TSCD layer\\nmodel is the Chinese scene text recognition CNN model with\\nTSCD layer in our previous work [39]. The residual network\\nmodel is a normal 20-layer model with three convolutional\\nblock sets, each set contains three blocks. The proposed\\n3200 VOLUME 5, 2017'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nfeature extractor model is the scene text recognition model\\nwith our proposed Chinese text structure feature extractor,\\nwhose detail is shown in Section IV .B.\\nIn the evaluation results, the text recognition model our\\nproposed Chinese text structure feature extractor achieves\\nbetter results in both datasets than other models and the OCR\\nsystem. The proposed feature extractor shows its advantage\\nin extracting accurate features for recognition.\\nIt is observed from the results that the text recognition\\nmodels trained with ‘‘S’’ performs much worse than the lead-\\ning results on both datasets, which demonstrates the scene\\ncharacters images extracted from available datasets are far\\nfrom enough to train a creditable deep text recognition model.\\nAnd the models with residual design perform worse than the\\nshallow TSCD layer based model when trained with ‘‘S.’’\\nThese results show that the residual design in deep model is\\nprone to over-ﬁtting with small training set.\\nWhen using the artiﬁcial character images, there is a huge\\ngap between the results on the two datasets. It is because\\nZhou’s dataset contains a number of artiﬁcial text images,\\nwhich is similar to the generated images. So its performance\\nhas huge improvement. And as the images in Ren’s dataset\\nare all scene images, the ‘‘A’’ models perform better than the\\n‘‘S’’ models but still much worse than the leading results.\\nModels trained with ‘‘A’’ training set formation have similar\\nrecognition performances. The results of TSCD layer model\\nand residual network model are almost equal. The proposed\\nmodel achieves better recognition accuracies in both dataset,\\nwhich preliminary shows effectiveness of the proposed Chi-\\nnese text structure feature extractor.\\nThe text recognition models trained with ‘‘A+S’’ have\\nconsiderable improvements than ‘‘A,’’ especially in Ren’s\\ndataset, which shows the importance of adding scene charac-\\nter images to train a creditable deep model. Among the mod-\\nels, TSCD layer model and residual network model achieve\\nsimilar results on both datasets. The TSCD layer model per-\\nforms slightly better than the residual network model, which\\nindicates priori knowledge structure has larger inﬂuence than\\nvery deep network in text recognition. The model with our\\nproposed Chinese text structure feature extractor achieves\\nmuch better accuracies than the other two models in ‘‘A+S,’’\\nwhich demonstrates that the TSCD layer and the residual\\nnetwork are organically combined in the proposed feature\\nextractor.\\nD. CHINESE SCENE TEXT DETECTION\\nThe scene text detection model with our proposed Chinese\\ntext structure feature extractor is evaluated on two multilin-\\ngual text datasets. Ren’s dataset contains language labels for\\nevery text lines, thus the Chinese text detection model can\\nbe evaluated more accurately by focusing on the Chinese\\ntext lines. Pan’s dataset does not contain language labels\\nbut has been widely used in many text detection algorithm\\nevaluations. On Pan’s dataset, the text detection model is\\nevaluated in wider language environment and compared to\\nmore text detection algorithms.\\nTABLE 3. Text detection results on Ren’s dataset.\\nTable III summarizes Chinese scene text recognition evalu-\\nation results with different deep models. The CNN model is a\\nbaseline model contains two convolutional layers, two down-\\nsampling layers, and a fully connected layer. The CSAE\\nmodel uses an unsupervised learning method, which is pro-\\nposed by us in [6], to pretrain the CNN model. The TSCD\\nlayer model has a TSCD layer in place of the second convo-\\nlutional layer in the CNN model. And the CSAE +TSCD\\nlayer model uses the CSAE unsupervised learning method to\\npretrain the TSCD layer model. The residual network model\\nis a normal 20-layer model with three convolutional block\\nsets, each set contains three blocks. The proposed model is\\nthe scene text detection model with our proposed Chinese\\ntext structure feature extractor, whose detail is shown in\\nSection IV .B. The evaluation method in this experiment is\\nthe same one we used in our previous work [39], which is\\nbased on the evaluation method of ICDAR 2011 and specially\\ndesigned for one-language text detection evaluation.\\nIn the Chinese scene text detection evaluation results of\\ndifferent deep models, the text detection model with our\\nproposed Chinese text structure feature extractor achieves the\\nbest performance of precision 0.87 and recall 0.82. The CSAE\\nunsupervised learning method is applied to extract more accu-\\nrate stroke features by pretraining the model. The residual\\nnetwork is also applied to extract more accurate stroke fea-\\ntures by computing feature residuals. Their evaluation results\\nare similar, but the deconvolution computing in the CSAE is\\nmuch more complex than the residual computing in the resid-\\nual network model. The TSCD layer model also achieves sim-\\nilar overall measurement as the CSAE and residual network\\nmodels do. And it can be noted that it has larger improvement\\nin precision than recall, which demonstrates the uniqueness of\\nChinese text structure features extracted by the TSCD layer.\\nCompared with the model that combines the CSAE method\\nand the TSCD layer, the model with our proposed Chinese\\ntext structure feature extractor achieves better results, even in\\nthe case where the feature extractor is not ﬁne-tuned in the\\ndetection model. It indicates that the design of the proposed\\nfeature extractor has good combination of the TSCD layer\\nand residual network. The results also demonstrate that the\\nextracted Chinese structure features are applicable to both\\ntext detection and recognition.\\nThe proposed Chinese scene text detection model is eval-\\nuated on Pan’s dataset which contains many Chinese text\\nregions and has been widely used in multilingual text detec-\\ntion. It enables the proposed model to be compared with more\\ntext detection algorithms in wider language environment.\\nAs most text regions in Pan’s dataset are Chinese text regions,\\nVOLUME 5, 2017 3201'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nFIGURE 7. The Chinese text information extraction result examples.\\nthe performance degradation of the proposed model is lim-\\nited. In this evaluation, the proposed Chinese text struc-\\nture feature extractor in the text detection model is also\\ntrained in the text recognition model in Ren’s dataset and not\\nﬁne-tuned.\\nTABLE 4. Text detection results with different algorithms.\\nTable IV summarizes the evaluation results of the proposed\\ntext detection model and some text detection algorithms on\\nPan’s dataset. Although many text regions in the dataset\\ndo not contain Chinese texts, which has negative effects\\non both of the measurements, the proposed model achieves\\nthe second best result. Compared to Tian’s algorithm, our\\nmodel has obvious weakness in recall, which is resulted by\\nthe unique Chinese structure we used in the model would\\ncause many English text regions to be detected as background\\nregions. The result implies that the uniqueness of the features\\nextracted by proposed Chinese text structure feature extractor\\nwould not be as effective in other language environments.\\nCompared with Liu’s algorithm, which is the best Chinese\\ntext detection algorithm on Pan’s dataset, our proposed model\\nachieves better results on both measurements, especially on\\nprecision. The results indicate that the Chinese text structure\\nfeature has advantages in text detection.\\nE. CHINESE SCENE TEXT INFORMATION EXTRACTION\\nWe design a Chinese scene text information extraction sys-\\ntem, which could be regarded as an embryonic form, by sim-\\nply connecting the text detection and recognition models.\\nIn this system, a text image is ﬁrstly divided to many image\\npatches by a multi-scale sliding window method. Secondly,\\nthe image patches are classiﬁed by the text detection model.\\nThen the image patches are merged into text lines by sev-\\neral geometric and heuristic rules such as similar colors\\nand horizontal distances. After the text lines are detected,\\nthey are divided to image patches again by a single-scale\\nsliding window method. Then the characters in the text line\\nare recognized individually by the text recognition model.\\nFinally, the characters are connected into recognized text\\nlines according to their positions and recognize scores.\\nIn the evaluation, three types of the embryonic sys-\\ntem, which are constructed with CNN models, TSCD layer\\nmodels and the proposed models, are evaluated on Ren’s\\ndataset [6] as it contains real and well labeled scene text\\nregions. The system with the proposed models achieves\\nthe best result P/R/F of 0.67/0.58/0.62, compared to the\\nresult of 0.64/0.52/0.57 with the TSCD layer models and\\n0.48/0.39/0.43 with normal CNN models. Some examples\\nof successful results are shown in Fig. 7. It can be noted\\nthat using text structure features (the TSCD layer and the\\nproposed feature extractor), the system performances are\\nsigniﬁcantly improved, which shows the potential of text\\nstructure features in unifying text detection and recognition.\\nAnd the system performers better with the proposed feature\\nextractor than the TSCD layer, which demonstrates the fea-\\ntures extracted by the proposed feature extractor are more\\naccurate than those extracted by TSCD layer based models.\\nV. CONCLUSION\\nIn this paper, we present a novel Chinese text structure fea-\\nture extractor for both text detection and recognition. It well\\nsimulates the key mechanisms in the three-layer Chinese\\ntext cognition model of human, which enables human detect\\nand recognize texts at the same time. In the Chinese text\\nstructure feature extractor, text structure component detector\\nlayer, which simulates the structure component layer in the\\npsychological model, and residual network, which simulates\\nthe bidirectional information transmission in the psycholog-\\nical model, are combined by a TSCD block. The design of\\nTSCD block incorporates the design ideas of the TSCD layer\\nand the residual network, which not only integrates them into\\na whole, but also enables them to complement each other.\\nTherefore, the features extracted by the TSCD block based\\nChinese text structure feature extractor are applicable to both\\nChinese text detection and recognition as we human do.\\nExperimental results demonstrate that the proposed Chinese\\ntext structure feature extractor is effective in both Chinese\\nscene text detection and recognition due to its high accuracy\\nand completeness in Chinese text structure feature extraction.\\nIt is also observed that adding artiﬁcial training samples\\nis essential in training the proposed feature extractor in a\\ntext recognition model as the existing training samples are\\nlimited. The feature extractor trained in the text recognition\\nmodel is effective in a text detection model even it is not ﬁne-\\ntuned for detection. And an embryonic form of Chinese scene\\ntext information extraction system using the feature extrac-\\ntor in both stages achieves promising results. Therefore, the\\n3202 VOLUME 5, 2017'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nproposed feature extractor is suitable to Chinese text infor-\\nmation extraction algorithms as a uniﬁed feature extractor for\\nboth text detection and recognition.\\nREFERENCES\\n[1] X. Chen and A. L. Yuille, ‘‘Detecting and reading text in natural scenes,’’ in\\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), vol. 2. Jul. 2004,\\npp. II-366–II-373.\\n[2] X. Li, W. Wang, S. Jiang, Q. Huang, and W. Gao, ‘‘Fast and effective text\\ndetection,’’ inProc. 15th IEEE Int. Conf. Image Process. (ICIP), Oct. 2008,\\npp. 969–972.\\n[3] C. Jung, Q. Liu, and J. Kim, ‘‘A stroke ﬁlter and its application to\\ntext localization,’’ Pattern Recognit. Lett., vol. 30, no. 2, pp. 114–122,\\n2009.\\n[4] B. Epshtein, E. Ofek, and Y . Wexler, ‘‘Detecting text in natural scenes\\nwith stroke width transform,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. (CVPR), Jun. 2010, pp. 2963–2970.\\n[5] Q. Ye, Q. Huang, W. Gao, and D. Zhao, ‘‘Fast and robust text detec-\\ntion in images and video frames,’’ Image Vis. Comput., vol. 23, no. 6,\\npp. 565–576, Jun. 2005.\\n[6] X. Ren, K. Chen, X. Yang, Y . Zhou, J. He, and J. Sun, ‘‘A new unsupervised\\nconvolutional neural network model for chinese scene text detection,’’\\nin Proc. IEEE China Summit Int. Conf. Signal Inf. Process. (ChinaSIP),\\nJul. 2015, pp. 428–432.\\n[7] W. Huang, Y . Qiao, and X. Tang, ‘‘Robust scene text detection with\\nconvolution neural network induced MSER trees,’’ in Proc. Eur. Conf.\\nComput. Vis. (ECCV), 2014, pp. 497–511.\\n[8] L. Wu, P. Shivakumara, T. Lu, and C. L. Tan, ‘‘A new technique for multi-\\noriented scene text line detection and tracking in video,’’ IEEE Trans.\\nMultimedia, vol. 17, no. 8, pp. 1137–1152, Jan. 2015.\\n[9] A. Mishra, K. Alahari, and C. Jawahar, ‘‘Scene text recognition using\\nhigher order language priors,’’ in Proc. Brit. Mach. Vis. Conf. (BMVC),\\n2012, p. 1.\\n[10] C. Yao, X. Bai, B. Shi, and W. Liu, ‘‘Strokelets: A learned multi-scale\\nrepresentation for scene text recognition,’’ in Proc. IEEE Conf. Comput.\\nVis. Pattern Recognit. (CVPR), Jun. 2014, pp. 4042–4049.\\n[11] I. J. Goodfellow, Y . Bulatov, J. Ibarz, S. Arnoud, and V . Shet,\\n(2013). ‘‘Multi-digit number recognition from street view imagery\\nusing deep convolutional neural networks,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1312.6082\\n[12] C. Shi, C. Wang, B. Xiao, Y . Zhang, S. Gao, and Z. Zhang, ‘‘Scene\\ntext recognition using part-based tree-structured character detection,’’\\nin Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Apr. 2013,\\npp. 2961–2968.\\n[13] T. Novikova, O. Barinova, P. Kohli, and V . Lempitsky, ‘‘Large-lexicon\\nattribute-consistent text recognition in natural images,’’ in Proc. Eur. Conf.\\nComput. Vis. (ECCV), 2012, pp. 752–765.\\n[14] K. Wang, B. Babenko, and S. Belongie, ‘‘End-to-end scene text recogni-\\ntion,’’ in Proc. Comput. Vis. (ICCV), 2011, pp. 1457–1464.\\n[15] L. Neumann and J. Matas, ‘‘On combining multiple segmentations\\nin scene text recognition,’’ in Proc. 12th Int. Conf. Document Anal.\\nRecognit. (ICDAR), Oct. 2013, pp. 523–527.\\n[16] T. Wang, D. J. Wu, A. Coates, and A. Y . Ng, ‘‘End-to-end text recognition\\nwith convolutional neural networks,’’ in Proc. 21st Int. Conf. Pattern\\nRecognit. (ICPR), Nov. 2012, pp. 3304–3308.\\n[17] C. Yao, X. Bai, and W. Liu, ‘‘A uniﬁed framework for multioriented text\\ndetection and recognition,’’ IEEE Trans. Image Process., vol. 23, no. 11,\\npp. 4737–4749, Jun. 2014.\\n[18] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman, ‘‘Reading\\ntext in the wild with convolutional neural networks,’’ Int. J. Comput. Vis.,\\nvol. 116, no. 1, pp. 1–20, 2016.\\n[19] Y . Zhu, C. Yao, and X. Bai, ‘‘Scene text detection and recognition:\\nRecent advances and future trends,’’ Frontiers Comput. Sci., vol. 10, no. 1,\\npp. 19–36, 2016.\\n[20] P. Shivakumara, W. Huang, T. Q. Phan, and C. L. Tan, ‘‘Accurate video text\\ndetection through classiﬁcation of low and high contrast images,’’ Pattern\\nRecognit., vol. 43, no. 6, pp. 2165–2185, Jun. 2010.\\n[21] P. Shivakumara, T. Q. Phan, and C. L. Tan, ‘‘A Laplacian approach to multi-\\noriented text detection in video,’’ IEEE Trans. Pattern Anal. Mach. Intell.,\\nvol. 33, no. 2, pp. 412–419, Feb. 2011.\\n[22] H. Chen, S. S. Tsai, G. Schroth, D. M. Chen, R. Grzeszczuk, and\\nB. Girod, ‘‘Robust text detection in natural images with edge-enhanced\\nmaximally stable extremal regions,’’ in Proc. 18th IEEE Int. Conf. Image\\nProcess. (ICIP), Sep. 2011, pp. 2609–2612.\\n[23] H. Cho, M. Sung, and B. Jun, ‘‘Canny text detector: Fast and robust scene\\ntext localization algorithm,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. (CVPR), Mar. 2016, pp. 3566–3573.\\n[24] C. Yao, X. Bai, W. Liu, Y . Ma, and Z. Tu, ‘‘Detecting texts of arbitrary\\norientations in natural images,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. (CVPR), Oct. 2012, pp. 1083–1090.\\n[25] L. Kang, Y . Li, and D. Doermann, ‘‘Orientation robust text line detec-\\ntion in natural images,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. (CVPR), Mar. 2014, pp. 4034–4041.\\n[26] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu, and X. Bai, (2016). ‘‘Multi-\\noriented text detection with fully convolutional networks,’’ [Online]. Avail-\\nable: https://arxiv.org/abs/1604.04018\\n[27] Q. Ye and D. Doermann, ‘‘Text detection and recognition in imagery:\\nA survey,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 7,\\npp. 1480–1500, Jul. 2015.\\n[28] C.-Y . Lee and S. Osindero, (2016). ‘‘Recursive recurrent nets with\\nattention modeling for OCR in the wild,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1603.03101\\n[29] B. Shi, X. Wang, P. Lv, C. Yao, and X. Bai, (2016). ‘‘Robust\\nscene text recognition with automatic rectiﬁcation,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1603.03915\\n[30] M. Jaderberg, A. Vedaldi, and A. Zisserman, ‘‘Deep features for text\\nspotting,’’ in Proc. Eur. Conf. Comput. Vis., 2014, pp. 512–528.\\n[31] X. Ren, K. Chen, X. Yang, Y . Zhou, J. He, and J. Sun, ‘‘A novel text struc-\\nture feature extractor for chinese scene text detection and recognition,’’ in\\nProc. 23nd Int. Conf. Pattern Recognit. (ICPR), Aug. 2016, pp. 36–45.\\n[32] K. He, X. Zhang, S. Ren, and J. Sun, (2015). ‘‘Deep residual learning for\\nimage recognition,’’ [Online]. Available: https://arxiv.org/abs/1512.03385\\n[33] M. Jaderber, K. Simonyan, A. Vedaldi and A. Zisserman. (2014). ‘‘Syn-\\nthetic data and artiﬁcial neural networks for natural scene text recogni-\\ntion.’’ [Online]. Available: https://arxiv.org/abs/1406.2227\\n[34] A. Gupta, A. Vedaldi, and A. Zisserman, (2016). ‘‘Synthetic\\ndata for text localisation in natural images,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1604.06646\\n[35] M. Taft and X. Zhu, ‘‘The representation of bound morphemes in the lexi-\\ncon: A Chinese study,’’ in Proc. Morphol. Aspects Lang. Process. (MALP),\\n1995, pp. 293–316.\\n[36] M. Shen, Z. Li, and Z. Zuxiang, ‘‘The inﬂuence of component startup on\\nthe recognition of ﬁtting Chinese characters,’’ Psychol. Sci., vol. 3, no. 5,\\npp. 206–211, 1997.\\n[37] J. Liu, S. Zhang, H. Li, and W. Liang, ‘‘A Chinese character localization\\nmethod based on intergrating structure and CC-clustering for advertis-\\ning images,’’ in Proc. Int. Conf. Document Anal. Recognit. (ICDAR),\\nSep. 2011, pp. 1044–1048.\\n[38] X. Zhou and Y . Li, ‘‘A research of chinese character utility function (in\\nchinese),’’Linguistic Res., vol. 1, no. 10, pp.62–65, 2009.\\n[39] X. Ren, Y . Zhou, J. He, K. Chen, X. Yang, and J. Sun, ‘‘A convolutional\\nneural network based Chinese text detection algorithm via text struc-\\nture modeling,’’ IEEE Trans. Multimedia, vol. 19, no. 3, pp. 506–518,\\nMar. 2017.\\n[40] X. Zhou, S. Zhou, C. Yao, Z. Cao, and Q. Yin, (2015). ‘‘Icdar\\n2015 text reading in the wild competition,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1506.03184\\n[41] Y .-F. Pan, X. Hou, and C.-L. Liu, ‘‘A hybrid approach to detect and localize\\ntexts in natural scene images,’’ IEEE Trans. Image Process., vol. 20, no. 3,\\npp. 800–813, Mar. 2011.\\n[42] A. Shahab, F. Shafait, and A. Dengel, ‘‘ICDAR 2011 robust reading\\ncompetition challenge 2: Reading text in scene images,’’ in Proc. Int. Conf.\\nDocument Anal. Recognit. (ICDAR), Mar. 2011, pp. 1491–1496.\\n[43] X.-C. Yin, X. Yin, K. Huang, and H.-W. Hao, ‘‘Robust text detection in\\nnatural scene images,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 36,\\nno. 5, pp. 970–983, Oct. 2014.\\n[44] S. Tian, Y . Pan, C. Huang, S. Lu, K. Yu, and C. L. Tan, ‘‘Text ﬂow: A\\nuniﬁed text detection system in natural scene images,’’ in Proc. IEEE Int.\\nConf. Comput. Vis. (ICCV), Dec. 2015, pp. 4651–4659.\\n[45] X. Liu, Z. Lu, J. Li, and W. Jiang, ‘‘Detection and segmentation text from\\nnatural scene images based on graph model,’’ in Proc. WSEAS Trans.\\nSignal Process., 2014, p. 10.\\nVOLUME 5, 2017 3203'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nXIAOHANG REN received the B.S. degree in\\nelectronic engineering from Zhejiang University,\\nHangzhou, China, in 2011. He is currently pur-\\nsuing the Ph.D. degree with the Department of\\nElectronic Engineering, Shanghai Jiao Tong Uni-\\nversity, Shanghai, China. His research interests\\ninclude text information extraction, deep learning\\nnetwork, and image retrieving.\\nYI ZHOUreceived the Ph.D. degree from Shang-\\nhai Jiao Tong University, China, in 2010. She is\\ncurrently with the Computer Science Department,\\nShanghai Jiaotong University, China. Her project\\nof Chinese characters reconginition is supported\\nby the National Science Foundation. Her major\\nresearch includes object recognition and big data\\nmining.\\nZHENG HUANG received the Ph.D. degree in\\ncomputer application technology from Shanghai\\nJiao Tong University, China, in 2003. He was\\na Computer Application Technology Professional\\nDoctorate with Shanghai Jiao Tong University\\nin 2003, where he is currently an Associate Pro-\\nfessor with the Institute of Information Security\\nand Engineering and also with the School of Infor-\\nmation Security Engineering. His research area is\\ninformation security and research interests include\\ncryptography and machine learning, information security, cryptography, and\\nmachine learning.\\nJUN SUN (M’06) received the B.S. degree in\\nelectrical engineering from the University of\\nElectronic Sciences and Technology of China,\\nChengdu, China, in 1989, and the Ph.D. degree\\nin electrical engineering from Shanghai Jiao Tong\\nUniversity, in 1995. In 1996, he was elected\\nas a member of the HDTV Technical Executive\\nExperts Group, China. Since then, he has been\\nacting as one of the main technical experts for the\\nChinese government in the ﬁeld of digital televi-\\nsion and multimedia communications. In the past ﬁve years, he has been\\nresponsible for several national projects in DTV and IPTV ﬁelds. He is\\ncurrently a Professor and Ph.D. Advisor with Shanghai Jiao Tong University.\\nHe has authored or co-authored over 50 technical papers in the area of\\ndigital television and multimedia communications. His research interests\\ninclude digital television, multimedia communication, and video encoding.\\nHe received the Second Prize of the National Science and Technology\\nDevelopment Award in 2003 and 2008.\\nXIAOKANG YANG (SM’14) received the\\nB.S. degree from Xiamen University, Xia-\\nmen, China, in 1994, the M.S. degree from\\nthe Chinese Academy of Sciences, Shang-\\nhai, China, in 1997, and the Ph.D. degree\\nfrom Shanghai Jiao Tong University, Shang-\\nhai, in 2000. From 2000 to 2002, he was a\\nResearch Fellow with the Center for Signal\\nProcessing, Nanyang Technological University,\\nSingapore. From 2002 to 2004, he was a Research\\nScientist with the Institute for Infocomm Research, Singapore. From 2007 to\\n2008, he visited the Institute for Computer Science, University of Freiburg,\\nGermany, as an Alexander von Humboldt Research Fellow. He is currently\\na Distinguished Professor with the School of Electronic Information and\\nElectrical Engineering and also the Deputy Director of the Institute of Image\\nCommunication and Information Processing, Shanghai Jiao Tong Univer-\\nsity. He has authored or co-authored over 200 refereed papers. He holds\\n40 patents. His current research interests include visual signal processing\\nand communication, media analysis and retrieval, and pattern recognition.\\nHe is a member of the Asia-Paciﬁc Signal and Information Processing\\nAssociation, a member of the Visual Signal Processing and Communications\\nTechnical Committee of the IEEE Circuits and Systems Society, a member\\nof the Multimedia Signal Processing Technical Committee of the IEEE\\nSignal Processing Society, the Chair of the Multimedia Big Data Interest\\nGroup of Multimedia Communications Technical Committee of the IEEE\\nCommunication Society. He was a member of the Editorial Board of the\\nDigital Signal Processing. He is also an Associate Editor of the IEEE\\nSIGNAL PROCESSING LETTERS and the Series Editor of the Communications in\\nComputer and Information Science(Springer).\\nKAI CHEN(M’09) received the Ph.D. degree from\\nShanghai Jiao Tong University, China, in 2003.\\nHe is currently with the Institute of Image Com-\\nmunication and Network Engineering, Shanghai\\nJiao Tong University. His major research includes\\ninformation retrieving, object recognition, and big\\ndata mining. He is the Key Member of the Insti-\\ntute on Network Engineering Research. He has\\nbeen involved with several key nation projects\\nand hosted many Industry-Academia Research\\nprojects.\\n3204 VOLUME 5, 2017')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader('sample.pdf')\n",
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9471e",
   "metadata": {},
   "source": [
    "### recursively split text by characters : RecursiveCharacter Text Splitters\n",
    "- text split will be done by list of characters \n",
    "- chunk size is measured by number of characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0d15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e957e55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Received February 15, 2017, accepted February 26, 2017, date of publication March 3, 2017, date of current version March 28, 2017.\\nDigital Object Identifier 10.1 109/ACCESS.2017.2676158\\nA Novel Text Structure Feature Extractor for\\nChinese Scene Text Detection and Recognition\\nXIAOHANG REN1, YI ZHOU1, ZHENG HUANG2, JUN SUN1, (Member, IEEE),\\nXIAOKANG YANG1, (Senior Member, IEEE), AND KAI CHEN1, (Member, IEEE)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='XIAOKANG YANG1, (Senior Member, IEEE), AND KAI CHEN1, (Member, IEEE)\\n1Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai 200240, China\\n2Institute of Information Security and Engineering, Shanghai Jiao Tong University, Shanghai 200240, China\\nCorresponding author: Y . Zhou (zy_21th@sjtu.edu.cn)'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Corresponding author: Y . Zhou (zy_21th@sjtu.edu.cn)\\nThis work was supported in part by the National Key Research and Development Program of China under Grant 2016YFB1001003, in part\\nby the National Natural Science Foundation of China under Grant 61521062 and Grant 61527804, in part by the Shanghai Science and\\nTechnology Committees of Scientiﬁc Research Project under Grant 14XD1402100 and Grant 15JC1401700.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='Technology Committees of Scientiﬁc Research Project under Grant 14XD1402100 and Grant 15JC1401700.\\nABSTRACT Scene text information extraction plays an important role in many computer vision applications.\\nMost features in existing text extraction algorithms are only applicable to one text extraction stage (text\\ndetection or recognition), which signiﬁcantly weakens the consistency in an end-to-end system, especially'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='for the complex Chinese texts. To tackle this challenging problem, we propose a novel text structure feature\\nextractor based on a text structure component detector (TSCD) layer and residual network for Chinese texts.\\nInspired by the three-layer Chinese text cognition model of a human, we combine the TSCD layer and\\nthe residual network to extract features suitable for both text extraction stages. The specialized modeling'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='for Chinese characters in the TSCD layer simulates the key structure component cognition layer in the\\npsychological model. And the residual mechanism in the residual network simulates the key bidirectional\\nconnection among the layers in the psychological model. Through the organic combination of the TSCD\\nlayer and the residual network, the extracted features are applicable to both text detection and recognition,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='as humans do. In evaluation, both text detection and recognition models based on our proposed text structure\\nfeature extractor achieve great improvements over baseline CNN models. And an end-to-end Chinese text\\ninformation extraction system is experimentally designed and evaluated, showing the advantage of the\\nproposed feature extractor as a uniﬁed feature extractor.\\nINDEX TERMS Text structure feature, Chinese text, deep learning, residual network, uniﬁed model.\\nI. INTRODUCTION'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='I. INTRODUCTION\\nText, an abstract presentation of artiﬁcial information, is scat-\\ntered throughout the human society in this succinct present-\\ning age. As portable digital recording devices are rapidly in\\nfashion among ordinary people, natural images and videos\\ncontents proliferate in image and video sharing websites, e.g.\\nYouTube and Flickr. By extracting text information, which\\ncarries high-level semantics, natural media contents can be'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='carries high-level semantics, natural media contents can be\\neffectively understood and used. It is crucial for a wide range\\nof applications such as image classiﬁcation, scene recogni-\\ntion and automatic navigation in urban environments.\\nWhile text recognition in scanned documents has been well\\nstudied and successfully deployed in real-world applications,\\nthe detection and recognition of texts in uncontrolled environ-\\nments still remains an open issue. Generally, text information'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='ments still remains an open issue. Generally, text information\\nextraction can be divided into two stages: text detection and\\ntext recognition. Traditionally, most works place the two\\ncomponents in consecutive stages, where text detection algo-\\nrithms commit to tackle the challenges of the variations of text\\nfont, size and style, complex backgrounds, noise, and uncon-\\nﬁrmed lighting conditions (like using ﬂash lamps) [1]–[8]\\nand text recognition algorithms of the huge variations of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='and text recognition algorithms of the huge variations of\\ntext layouts, orientations, geometric distortions and partial\\nocclusions [9]–[13]. These challenges have spawned various\\nfeature designs to meet the needs of one stage. Recently,\\na variety of end-to-end text information extraction algo-\\nrithms [14]–[18] are proposed that unify the features for both\\nstages to remove the error propagation between them, leading\\nto signiﬁcant improvements.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='stages to remove the error propagation between them, leading\\nto signiﬁcant improvements.\\nHowever, existing works still have certain limitations.\\nIn particular, most of them only focus on English texts,\\nwhich are relatively easy to deﬁne and recognize due to the\\nsimplicity in strokes and structures. In this age of globaliza-\\ntion, the recognition of multilingual texts attracts increasing\\nVOLUME 5, 2017\\n2169-3536'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='tion, the recognition of multilingual texts attracts increasing\\nVOLUME 5, 2017\\n2169-3536 \\n 2017 IEEE. Translations and content mining are permitted for academic research only.\\nPersonal use is also permitted, but republication/redistribution requires IEEE permission.\\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\n3193'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\ninterests. Among them, the logographic text is the most spe-\\ncial text type, which presents both pronunciation and meaning\\nin its appearance. As a typical logographic text, Chinese\\nreveals signiﬁcant different properties than English, which is\\na typical Latin-based text and has been well studied, from the\\nfollowing aspects:\\n1. Number of strokes. Most Chinese characters contain'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='following aspects:\\n1. Number of strokes. Most Chinese characters contain\\nmore than ﬁve strokes, while the most complex English\\ncharacter only has four.\\n2. Type of strokes. There are more than 30 different types\\nof Chinese strokes, while only 10 different types of\\nstrokes exist in English.\\n3. Style of characters. It is widely acknowledged that most\\nChinese characters are picto-phonetic, while English\\nuses abstract characters.\\n4. Intra-character structures. Chinese characters are more'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='uses abstract characters.\\n4. Intra-character structures. Chinese characters are more\\ncomplicated than English, where certain structures may\\nexist within a character.\\nDue to the aforementioned differences between Chinese\\nand English texts, the study of Chinese text recognition is\\nof much value in both theoretical and practical perspec-\\ntives. Unfortunately, current state-of-the-art text informa-\\ntion extraction algorithms for English texts cannot be easily'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='tion extraction algorithms for English texts cannot be easily\\ndeployed to Chinese. Because of the complexity of Chinese\\ntexts including the strokes and intro-character structures, it is\\nextremely difﬁcult for an algorithm to combine text detection\\nand recognition in a uniﬁed framework.\\nIn this paper, a Chinese text structure feature extractor\\nis designed, trained and applied to both text detection and\\nrecognition, making a number of key contributions.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='recognition, making a number of key contributions.\\nOur main contribution is a novel Chinese text structure\\nfeature extractor. Motivated by the three-layer Chinese text\\ncognition model of human in psychology, we combine the\\ntext structure component detector (TSCD) layer and the resid-\\nual network to simulate the two key mechanisms in the three-\\nlayer model. The TSCD layer has specialized modeling for\\nChinese structure components that are the bridge between'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='Chinese structure components that are the bridge between\\nstrokes and characters and play key role in the three-layer\\nmodel. The residual network has its unique residual mech-\\nanism that is an effective bidirectional information transmis-\\nsion between the upper and lower layers. It is highly similar\\nto the key bidirectional connection in the three-layer model.\\nBy reconstructing the components in the TSCD layer into\\na TSCD block (shown in Fig. 1) refer to the design idea'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='a TSCD block (shown in Fig. 1) refer to the design idea\\nof residual network, the organic combination is established\\nand the three-layer Chinese text cognition model is well\\nsimulated. Therefore, the extracted features are applicable to\\nboth stages in Chinese text information extraction.\\nOur second contribution is unifying Chinese structure fea-\\nture extractor in both text detection and recognition with\\nsharing trained parameters. In deep learning models, both text'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='sharing trained parameters. In deep learning models, both text\\ndetection and recognition are regarded as classiﬁcation tasks.\\nIn our detection and recognition models, the feature extractor\\nis trained in the recognition model, which focuses more on\\nthe structure features in Chinese characters. The structure\\nfeatures are unique and rarely seen in background regions,\\nFIGURE 1. The TSCD block.\\nthus they are applicable to distinguish text and background'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='FIGURE 1. The TSCD block.\\nthus they are applicable to distinguish text and background\\nregions without ﬁne-turning the extractor parameters.\\nOur third contribution is a synthetic data engine. As the size\\nof public available Chinese scene text datasets is not enough\\nto train deep models, it is essential to expand the training\\ndata with artiﬁcial samples. The synthetic data engine is\\ncomposed of three stages, each of them simulates one scene'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='composed of three stages, each of them simulates one scene\\ntext characteristic. Thus, the generated samples can partially\\nsubstitute the scene text images and become an essential\\ndata component in training the Chinese text structure feature\\nextractor.\\nThe rest of the document is organized as follows.\\nIn Section II, we introduce the related works of our work.\\nIn Section III, we describe the proposed Chinese text structure\\nfeature extractor. In Section IV , we present the experimental'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='feature extractor. In Section IV , we present the experimental\\nevaluation setting up, results and discussions. The paper is\\nconcluded in Section V .\\nII. RELATED WORKS\\nGenerally, text information extraction is divided into two\\nstages: text detection and text recognition. As text recogni-\\ntion in scanned documents is well studied and many OCR\\nsystems have achieved quite good performance, most existing\\nresearches focus on the text detection stage.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='researches focus on the text detection stage.\\nText detection approaches concern how to discover and\\nlocate the regions containing texts from scene images [19].\\nTraditionally, there are two major categories in text detec-\\ntion algorithms: region-based and texture-based. The sym-\\nbolic component of region-based algorithms is the image\\nregion extraction that limits the feature analyzation on image\\npatches. The features in these algorithms are always unique'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='patches. The features in these algorithms are always unique\\nin scene text regions. Sliding window based algorithms\\nand connected component (CC)-based algorithms are the\\nmain region-based algorithm types. Sliding window based\\n3194 VOLUME 5, 2017'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nalgorithms extract numerous overlapping image rectangles\\nfor feature extraction [2], [14], [20]. CC-based algorithms\\nextract connected image regions with uncertain shapes\\nand use a set of rules to identify scene text regions [3],\\n[4], [21]–[23]. The symbolic component of texture-based\\nalgorithms is the global texture features in the entire scene'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='algorithms is the global texture features in the entire scene\\nimage. By analyzing the distribution of the global texture\\nfeatures, text regions are extracted from the scene image\\nand connected into text lines. Machine learning methods are\\nthe most popular tools in texture-based algorithms, which\\nhave strong capability to extract text regions with the global\\ntexture features [1], [5], [7]. As scene texts can be in various\\norientations, multi-orientation approaches provide a practical'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='orientations, multi-orientation approaches provide a practical\\nsolution to scene text detection. Component analyzing is the\\ncore in recent multi-orientation approaches, including com-\\nponent aggregating, clustering and linking [24]–[26].\\nText recognition aims to extract text content information\\nfrom the cropped text images. As different language texts\\nare in great disparities, most existing scene text recognition\\nalgorithms focus solely on recognizing one language texts.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='algorithms focus solely on recognizing one language texts.\\nAmong them, English text recognition is widely studied\\nand can be classiﬁed into two major categories: character\\nbased recognition and word based recognition [27]. Character\\nbased recognition recognizes the characters in the image by\\na character classiﬁer and combine the recognize results into\\nwords. As English character types are very limited, most\\ncommon classiﬁers are competent to the task with appropriate'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='common classiﬁers are competent to the task with appropriate\\nfeatures [10], [12], [28]. Word based recognition directly\\nrecognizes the word in the text image. As there are numerous\\nEnglish words, common classiﬁers or models are not com-\\npetent to recognize words directly. So in some algorithms,\\nthe efﬁcient components of character based recognition are\\nstill adopted [9], [13], [29].\\nThe strategy of performing text detection and text recog-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='still adopted [9], [13], [29].\\nThe strategy of performing text detection and text recog-\\nnition in separate and independent stages may have certain\\nproblems. As the goal of text detection and the requirement\\nof text recognition occur discrepancies in many aspects, it is\\nargued that additional challenges may turn up when exploit-\\ning the text detection results for text recognition. Therefore,\\na variety of end-to-end text information extraction algo-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='a variety of end-to-end text information extraction algo-\\nrithms [14], [17], [18], [30] are proposed to combine both text\\ndetection and recognition stages by a uniﬁed framework. The\\nkey mechanism in the uniﬁed framework is uniﬁed features\\nwhich are applicable to both text detection and recognition.\\nDue to the higher complexity of Chinese texts than English\\ntexts, the design of uniﬁed features is much more difﬁcult.\\nIn our previous work [31], a TSCD layer is designed to extract'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='In our previous work [31], a TSCD layer is designed to extract\\nChinese structure features in CNN based model. The structure\\nfeatures are essential in Chinese characters, thus they are\\napplicable for both Chinese text detection and recognition.\\nRecently residual network proposed by He et al. [32] is\\nunder the spotlight in deep learning researchers. They design\\nseveral convolutional blocks with residual compute and feed-\\nback routes, which enable the top parameters in a very deep'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='back routes, which enable the top parameters in a very deep\\nmodel learned effectively by current training method. Ben-\\neﬁt from the convolutional blocks, very deep convolutional\\nneuronal networks, e.g. 152 layers and 1001 layers, can be\\neasily designed. And they have achieved huge improvements\\nin classiﬁcation, detection and segmentation tasks. Further-\\nmore, the convolutional blocks also quicken the bidirec-\\ntional information transmission route between layers. Thus'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='tional information transmission route between layers. Thus\\nthe design of convolutional block is instructive for other tasks\\nthat require bidirectional information transmission.\\nSynthetic data engines for artiﬁcial text image generation\\nachieve great success in deep learning based scene English\\ntext information extraction as the size of public available\\nEnglish text dataset is not enough for training a deep model.\\nIn [16], numerous simple synthetic text images are gener-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='In [16], numerous simple synthetic text images are gener-\\nated and used to train text detection and recognition models.\\nA complete synthetic data engine is designed in [33], which\\nsimulates English word regions in scene images. In [34], a fast\\nand scalable synthetic data engine that naturally blends text in\\nexisting natural scenes is proposed to generate scene images\\nwith text regions. As there are fewer public available Chinese\\ntext datasets and Chinese texts are more complex, it is in need'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='text datasets and Chinese texts are more complex, it is in need\\nof a synthetic data engine for Chinese texts.\\nIII. CHINESE TEXT STRUCTURE FEATURE EXTRACTOR\\nThe design of the proposed Chinese text structure feature\\nextractor is motivated by the three-layer Chinese text cog-\\nnition model of human, which is described in Section III.A.\\nThe text structure component detector (TSCD) layer and\\nthe residual network simulate the key mechanisms in the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='the residual network simulate the key mechanisms in the\\nmodel: structure component layer, which is the bridge in\\nthe model, and the bidirectional information transmission\\nbetween the upper and lower layers, respectively. Based on\\ntheir design ideas, a TSCD block is constructed with various\\nstructure feature extractors in residual information transmis-\\nsion route, which is described in Section III.B. Replacing a\\nset of convolutional blocks in a residual network model with'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='set of convolutional blocks in a residual network model with\\nTSCD blocks, Chinese text structure feature can be extracted\\naccurately to meet the requirements of both text detection\\nand recognition. In order to enhance the unity of the text\\ndetection and recognition models, the Chinese text structure\\nfeature is applied as uniﬁed features in the two models, whose\\nsharing structure is described in Section III.C. The network\\nparameters of the uniﬁed feature extractor are learned solely'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='parameters of the uniﬁed feature extractor are learned solely\\nin recognition model with a training set consisting of scene\\nand artiﬁcial samples. A considerable number of samples\\nin the training set are artiﬁcial text images generated by a\\nsynthetic data engine proposed in Section III.D. The engine\\ncontains three generation stages, which cover every essential\\nprocess from putting a text in the scene to taking a text\\nimage, to ensure the effectiveness of the generated samples\\nin training.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='image, to ensure the effectiveness of the generated samples\\nin training.\\nA. CHINESE TEXT COGNITION MODEL OF HUMAN\\nWhen human extract text information from vision, text detec-\\ntion and recognition stages are not independent and sequen-\\ntial. They work at the same time and complement each other:\\ndetection process assists recognition, and recognition pro-\\ncessing assists detection at the meanwhile. Thus we believe\\nVOLUME 5, 2017 3195'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nthat the extracted features in human cognition system for text\\nare applicable to both detection and recognition. By analyzing\\nand simulating the Chinese text cognition model of human,\\nwe can design a feature extractor for both text detection and\\nrecognition.\\nBased on the Chinese text recognition multi-layer activa-\\ntion model proposed by Taft and Zhu [35], Shen et al.[36]'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='tion model proposed by Taft and Zhu [35], Shen et al.[36]\\nproposed a three-layer Chinese text cognition model of\\nhuman, which has been widely accepted by the psychology\\ncommunity. In the cognition model (shown in Fig. 2), the Chi-\\nnese text cognitive process can be divided into three layers:\\ntext layer in the top, text structure component layer in the mid-\\ndle and stroke layer in the bottom. Psychological experiments\\nshow that the information transmission between two adjoin-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='show that the information transmission between two adjoin-\\ning layers is bidirectional, which has top-down and bottom-up\\ntwo transmission directions (represented by the arrow direc-\\ntions in Fig. 2). Furthermore, the bidirectional information\\ntransmission can be excitatory and inhibitory (represented by\\nthe solid and dashed arrows in Fig. 2).\\nFIGURE 2. The cognition model.\\nIt can be seen from the model, as the middle layer of the\\nthree-layer model, the text structure component has direct'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='three-layer model, the text structure component has direct\\nconnection and interaction to upper text and lower stroke.\\nSo the structure component is the bridge and key structure\\nin the Chinese text cognition model of human. And the\\nbidirectional connections strengthen the speed and ﬂexibility\\nof information transmission. So the bidirectional information\\ntransmission is the key transmission in the Chinese text cog-\\nnition model of human. Therefore, the key to simulate the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='nition model of human. Therefore, the key to simulate the\\nhuman text cognition feature lies in the simulation of the two\\nkey mechanisms: structure and information transmission.\\nB. TEXT STRUCTURE COMPONENT\\nDETECTOR (TSCD) BLOCK\\nChinese character is a kind of pictographs, which contains a\\nlarge number of radicals and structures. Compared with other\\nnon-Latin language scripts, i.e. Japanese, Koran and Arabic\\nscripts, Chinese script has richer character structures, which'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='scripts, Chinese script has richer character structures, which\\nmakes the structure features more important in Chinese script\\nthan other language scripts. There are many structure types in\\nmodern Chinese characters, which contain four basic types:\\nleft-right structure, top-bottom structure, inner-outer struc-\\nture and single character [37] and their sub-types such as left-\\nmiddle-right, top-left-right and so on. Each Chinese character\\nis comprised of several structure components according to its'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='is comprised of several structure components according to its\\nstructure type. Following the utility function proposed in [38],\\nChinese character structure components are extracted from\\nthe most commonly-used 1290 Chinese characters based on\\ntheir basic structure types. Although Chinese characters con-\\ntain a wide variety of the structure components in appearance,\\ntheir aspect ratio types are limited by their structures. Thus\\nthe structure components are able to be classiﬁed into a few'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='the structure components are able to be classiﬁed into a few\\ntypes according to their aspect ratio types. An example of the\\nstatistical result of top-bottom structure is shown in Fig. 3,\\nin which structure components extracted from top-bottom\\nstructure characters are classiﬁed into 10 types. It can be\\nseen from the statistical result that the top-bottom structure\\ncharacters contain three main structure component types(3:1,\\n3:2 and 2:1) and three secondary types(1:2, 1:1 and 2:3).'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='3:2 and 2:1) and three secondary types(1:2, 1:1 and 2:3).\\nAccording to the aspect ratio types of Chinese char-\\nacter structure components, several text structure compo-\\nnent detectors are designed in convolutional neural network.\\nAccording to its target character structure component type,\\nthe aspect ratio of the convolutional window in a text structure\\ncomponent detector is set as the same. The longer edge of the\\nconvolutional window is ﬁxed. Convolutional window deter-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='convolutional window is ﬁxed. Convolutional window deter-\\nmines what kind of feature the convolutional feature extrac-\\ntion is adapt in. For example, a 2:1 convolutional window is\\nmuch more sensitive to structure components with 2:1 aspect\\nratio than other convolutional windows and is insensitive\\nto structure components with other aspect ratios. Thus it\\nwill focus on extracting accurate features from 2:1 structure\\ncomponents after effective training. Those text structure com-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='components after effective training. Those text structure com-\\nponent detectors with various convolutional window sizes\\nextract structure component features in parallel and form a\\nTSCD layer. Thus, the TSCD layer well simulates the struc-\\nture of structure component layer in the three-layer Chinese\\ntext cognition model of human. More details of TSCD layer\\nare proposed in our previous work [39].\\nIn residual network [32], the basic units are residual con-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='are proposed in our previous work [39].\\nIn residual network [32], the basic units are residual con-\\nvolutional blocks. A residual convolutional block contains\\nseveral convolutional layers, batch normalization layers and a\\nshortcut route. The input features are separated to the shortcut\\nroute and the layers, whose functionality is to compute the\\nresidual of the feature. And the input features and the residual\\nfeatures are merged in the end. The information transmis-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='features are merged in the end. The information transmis-\\nsion in a residual convolutional block is quick and ﬂexible\\ndue to the shortcut route and the residual computing and\\nmerging. And because of forward and backward propaga-\\ntions in deep network models, the information transmission\\nis bidirectional. Thus, the residual network well simulates\\nthe information transmission in the three-layer Chinese text\\ncognition model of human.\\nThe TSCD layer and the residual network both simulate'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='cognition model of human.\\nThe TSCD layer and the residual network both simulate\\none of the two key mechanisms in the three-layer Chinese\\ntext cognition model of human. By effectively combining\\n3196 VOLUME 5, 2017'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nFIGURE 3. The statistical result of top-bottom structure characters.\\nFIGURE 4. The statistical result of Chinese character structure component aspect ratio types.\\nthem, the three-layer model can be well simulated and image\\nfeatures applicable to both Chinese text detection and recog-\\nnition can be extracted accurately. However, though they are'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='nition can be extracted accurately. However, though they are\\nboth based on CNN, their design directions are completely\\ndifferent. Simply connecting them into a CNN model cannot\\nequip the model with the two key mechanisms. Thus, we com-\\nbine them in design idea, to construct a convolutional struc-\\nture that not only has a feature extraction capacity of various\\nstructure component types as the TSCD layer, but also has\\na quick and ﬂexible information transmission as the residual'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='a quick and ﬂexible information transmission as the residual\\nnetwork. The design idea is to reconstruct the convolutional\\nblock of residual network with more convolutional window\\ntypes and place their residual computations in parallel, which\\nextracts various Chinese text structure features exclusively\\nand keeps their independence for accuracy.\\nBased on the above design ideas, we propose a TSCD block\\nto completely combine the TSCD layer and residual network.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='to completely combine the TSCD layer and residual network.\\nThe TSCD block is a special layer combination in CNN\\nmodel, whose structure overview is shown in Fig. 1. In the\\nTSCD block, the various components in the TSCD layer are\\nbroken up and reconstructed in parallel with batch normaliza-\\ntion and activation to compute their residual independently.\\nThe activation function is Rectiﬁed Linear Units (ReLu),\\nwhich simulates the sparse activation of human neurons.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='which simulates the sparse activation of human neurons.\\nUsing linear activation function also alleviates the Vanish-\\ning Gradient Problem in training. As there is no correlation\\namong the structure component types, the residuals of each\\nfeature types should be computed individually to keep their\\nindependence, which also reduces the computational com-\\nplexity. After the residual computations are completed, they\\nare connected into undivided features, which has the same'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='are connected into undivided features, which has the same\\nsize as the input features, using concat function. In this way,\\nthe inﬂuences among different component types in residual\\ncomputations are avoided. Finally, the input features and the\\nresidual features are merged into the output features as they\\ndo in a convolutional block in residual network. It should be\\nnoted that if the input features are stroke features extracted by\\na convolutional block rather than structure features extracted'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='a convolutional block rather than structure features extracted\\nby a TSCD block, there will be a TSCD layer in the shortcut\\nroute to extract structure features. The feature type order and\\nsizes in this TSCD layer are exactly the same as those in the\\nresidual computing route of the TSCD block, which ensures\\nthe input features and the residual structure features are\\ncorrectly matched according to their structure types before\\nmerging.\\nThrough the statistical result (as shown in Fig. 4), the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='merging.\\nThrough the statistical result (as shown in Fig. 4), the\\nChinese text structure component types to be detected and\\nextracted in the TSCD block can be determined. Those struc-\\nture component types are extracted from the most commonly-\\nused 1290 Chinese characters and classiﬁed by their aspect\\nratios. More than 99% structure components can be classiﬁed\\nin to the 11 types shown in the statistical result. Among them,\\nthe least three structure component types accounted for less'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='the least three structure component types accounted for less\\nthan 1% of the total. Thus, we choose the most common\\neight structure component types to be detected and extracted\\nfeatures in the TSCD block. To balance the feature inﬂuences\\nof different structure component types in the extracted fea-\\ntures, the feature sizes have the same proportions as their cor-\\nresponding structure component types. With such a design,\\nthe quantitative distribution of text structure features matches'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='the quantitative distribution of text structure features matches\\nthat of natural Chinese text structure component types, which\\ngrantees the most Chinese character structure components\\ncan be detected and extracted features in the TSCD block.\\nOur proposed Chinese text structure feature extractor is\\nconstructed by two convolutional block sets and one TSCD\\nblock set. The two convolutional block sets are applied to\\nVOLUME 5, 2017 3197'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nextract accurate stroke features from input image. The TSCD\\nblock set is applied to extract speciﬁed Chinese text structure\\nfeatures from the stroke features with bidirectional informa-\\ntion transmission. In this way the two lower layers in three-\\nlayer Chinese text cognition model of human are simulated\\nin a deep learning model based on their two key mechanisms:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='in a deep learning model based on their two key mechanisms:\\nstructure and information transmission. Thus, the extract fea-\\ntures are applicable to both text detection and recognition.\\nC. UNIFIED FEATURES IN TEXT DETECTION AND\\nRECOGNITION MODELS\\nIn current deep learning model design, it is difﬁcult and costly\\nto get detection and recognition results from an output layer.\\nSo we do not unify the Chinese text detection and recognition\\nmodels from top to bottom, but we input the features into'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='models from top to bottom, but we input the features into\\ndifferent fully connected and output layers after the uniﬁed\\nfeature extraction.\\nFIGURE 5. The unified Chinese structure feature extractor in text\\ndetection and recognition models.\\nFig. 5 shows the overview of the uniﬁed Chinese\\nstructure feature extractor in text detection and recognition\\nmodels. In the top, the proposed Chinese text structure feature\\nextractor is applied to extract the uniﬁed feature from input'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='extractor is applied to extract the uniﬁed feature from input\\nimage patches. Then the features are ﬂattened and inputted to\\ntwo different analyzing structures. One is the text detection\\nstructure, which contains a fully connected layer to analyze\\nthe features for text detection and a binary softmax classiﬁer\\nto output the detection result of whether the input image\\npatch is a text patch or a background patch. The other is the\\ntext recognition structure, which contains a fully connected'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='text recognition structure, which contains a fully connected\\nlayer to analyze the features for text recognition and a multi-\\nclass softmax classiﬁer to output the recognition result of\\nwhich Chinese character the input image patch represents.\\nIn the view of text information system, these structures can\\nbe regarded as sharing feature extraction in text detection and\\nrecognition models.\\nAs text recognition requires more accurate and unique text'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='recognition models.\\nAs text recognition requires more accurate and unique text\\nstructure features than text detection, the shared Chinese text\\nstructure feature extractor is trained in the text recognition\\nmodel with text images. Beneﬁt from the unique text structure\\nfeatures in Chinese, the extracted features for text recognition\\nwork well in text detection model. So the feature extractor\\ntrained for text recognition has not to be ﬁne-tuned for text'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='trained for text recognition has not to be ﬁne-tuned for text\\ndetection, which simpliﬁes the training process and reduces\\nthe computational complexity of detection and recognition.\\nDue to the sharing mechanism, the model unity is signiﬁ-\\ncantly improved than that in [31], which has uniﬁed structure\\nof feature extraction.\\nD. SYNTHETIC DATA ENGINE\\nChinese text is more complex than English text in stroke,\\nstyle and structure, which imposes higher requirements on the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='style and structure, which imposes higher requirements on the\\nquantity and quality of training images to train credible deep\\nmodels. However, the size of public available scene Chinese\\nscene text datasets is much smaller than English text and far\\nfrom enough to satisfy the requirement. Inspired by the suc-\\ncessful use of artiﬁcial text images in English text information\\nextraction deep model [18], we design a synthetic data engine\\nto generate high quality artiﬁcial Chinese character images.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='to generate high quality artiﬁcial Chinese character images.\\nConsidering the high similarity of Chinese fonts, charac-\\nters are generated by several representative Chinese fonts\\nrather than all the available fonts. Meanwhile, as Chinese\\ncharacters are all square shaped, we randomly assign the\\ncharacter size in generation. Our synthetic data engine con-\\nsists of three stages: character image generation, image trans-\\nformation, camera inﬂuence generation. Their details are as\\nfollows:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='formation, camera inﬂuence generation. Their details are as\\nfollows:\\n1. Character image generation: The character image are\\ngenerated as we usually do when printing characters\\nin nature. There are three steps in this stage. Firstly,\\ncharacters are generated by Chinese fonts. As many\\nChinese fonts are highly similar, we collect 32 Chinese\\nfonts, which includes three basic fonts Kai, Song and\\nHei, 24 derived fonts such as FangSong, XiHei and ﬁve\\ncommon special fonts, to generate characters. (When'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='common special fonts, to generate characters. (When\\ngenerating characters, 30% of them are basic fonts,\\n60% are derived fonts and 10% are special fonts.)\\nSecondly, the character color and background color are\\nrandomly selected form the most common 27 colors in\\nscene text images. Finally, borders with random thick-\\nness and color are added randomly to the characters.\\n2. Image transformation: To simulate the characters in\\nscene environment, some natural transformations are'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='scene environment, some natural transformations are\\nadded to the generated character images. There are\\nthree steps in this stage. Firstly, shadows with ran-\\ndom orientation, thickness and intention are added\\nrandomly to the characters, which simulates nature\\nlight. Secondly, full-projective transformation method\\nare used the transform the image with random\\nparameters, which simulates different viewpoints of\\nobserver. Finally, scene background image patches are'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='observer. Finally, scene background image patches are\\nblended to the characters with random blend inten-\\nsity and blend mode, which simulates the reﬂection in\\nnature.\\n3. Camera inﬂuence generation: In this stage, some types\\nof image noise, which is common in taking pictures\\nfrom natural, are added to the image. In our engine,\\n3198 VOLUME 5, 2017'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nwe add Gaussian noise and Gaussian blur to the char-\\nacter image with random intensity.\\nThe representative Chinese character images generated by\\nour synthetic data engine with random parameters perfectly\\nsimulates the text regions in scene images. And they become\\nessential components in the training set to train our proposed\\nChinese text structure feature extractor.\\nIV. EXPERIMENTS'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='Chinese text structure feature extractor.\\nIV. EXPERIMENTS\\nIn this section the proposed Chinese text structure feature\\nextractor is evaluated with scene text detection and recog-\\nnition models. The feature extractor is ﬁrst evaluated with\\ntext recognition model as it is trained in the model. Then\\nit is evaluated with text detection model. Finally the detec-\\ntion and recognition models are connected and the proposed\\nfeature extractor is evaluated as a uniﬁed feature extractor.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='feature extractor is evaluated as a uniﬁed feature extractor.\\nThree multilingual text datasets [6], [40], [41] are used in the\\nevaluations, one of them is used in all the evaluations and\\nthe other two are used in either text detection or recognition\\nevaluation.\\nIn Section IV .A, the datasets used in evaluations are\\npresented. Then the details of training samples, deep\\nmodels, training methods are introduced in section IV .B.\\nIn section IV .C, the text recognition evaluation results are'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='In section IV .C, the text recognition evaluation results are\\npresented and discussed. The text detection evaluation results\\nare presented and discussed in section IV .D. The evaluation\\nresults of connected models are presented and discussed\\nin section IV .E.\\nA. DATASETS\\nThree multilingual datasets are used to evaluate our pro-\\nposed Chinese text structure in different models. Among\\nthem, Ren’s dataset [6] is used in all the evaluations. Zhou’s'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='them, Ren’s dataset [6] is used in all the evaluations. Zhou’s\\ndataset [40] is used in the evaluation of text recognition and\\nPan’s dataset [41] is used in the evaluation of text detection.\\nA summary of the datasets is shown in Table I.\\nTABLE 1. A description of the various datasets evaluated on.\\nRen’s dataset is a multilingual scene text detection and\\nrecognition dataset proposed by us in 2015. There are\\n194 images in the training set and 200 images in the testing'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='194 images in the training set and 200 images in the testing\\nset. All the training images and half of the testing images are\\ntaken from open ﬁelds with different weathers and indoor\\nwith different lights from various objects by a camera. The\\nother half of the testing images are taken by various photogra-\\nphers on the Internet. All of them are scene images and taken\\nby various shutterbugs in natural. And all the text regions\\nhave language labels to improve the evaluation accuracy of'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='have language labels to improve the evaluation accuracy of\\nparticular language types.\\nZhou’s dataset is a multilingual text detection and recog-\\nnition dataset proposed in 2015. The training set contains\\n483 images and the testing set contains 484 images. It is\\nnoted that not all the images in the dataset are scene images.\\nA considerable amount of images are artiﬁcial text images\\nincluding Internet advertisements and screenshots.\\nPan’s dataset is a multilingual scene text detection dataset'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='Pan’s dataset is a multilingual scene text detection dataset\\nproposed in 2011. It is popular in multilingual text detection\\nalgorithm evaluations. There are 248 images in the training\\nset and 239 images in the testing set. It is noted that the\\ntext regions in the dataset do not have language labels. Thus,\\nit can only evaluate the general multilingual text detection\\nperformance with all language types.\\nCompared with other image detection and recognition'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='performance with all language types.\\nCompared with other image detection and recognition\\ndatasets, scene text detection and recognition datasets are\\nsmaller, in which training set and testing set generally contain\\nonly a few hundred images. However, because many text\\nand text regions are contained in a scene image, hundreds of\\ntraining and test images are sufﬁcient to comprehensively and\\neffectively evaluate text detection and recognition algorithms.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='effectively evaluate text detection and recognition algorithms.\\nCompared with the most widely used text detection dataset\\nICDAR 2011 [42], which contain 229 train images and\\n255 test images, the above datasets contain similar or more\\nscene text images. Note that although Ren’s dataset contains\\na bit less (194, 200) images, it is expected that using our\\ndataset can be more effective for evaluation of our proposed\\nChinese text detection and recognition models as the labels'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='Chinese text detection and recognition models as the labels\\nin our dataset are more comprehensive.\\nFIGURE 6. The training samples. (a) The examples of the artificial image\\npart. (b) The examples of the natural image part.\\nB. TRAINING DETAILS\\n1) TRAINING SAMPLES\\nThe training samples to train the proposed Chinese text\\nstructure feature extractor in the text recognition model are\\ncomposed of two parts (examples are shown on Fig. 6). One\\npart is the artiﬁcial Chinese character scene images gener-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='part is the artiﬁcial Chinese character scene images gener-\\nated by our proposed synthetic data engine (Section III.E).\\nThere are 96,000 artiﬁcial character images generated with\\n1500 Chinese characters(64 images each) which are com-\\nposed of the most commonly used characters and the char-\\nacters in the multilingual datasets. The other part is the\\nscene character images extracted from the training sets of\\nthe multilingual datasets. The models evaluated in different'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='the multilingual datasets. The models evaluated in different\\ndatasets are trained with different scene training sets. There\\nare 6000 character images extracted in Ren’s dataset and\\n7000 character images extracted in Zhou’s dataset.\\nThe training samples to train the text detection model with\\nthe proposed Chinese text structure feature extractor are the\\nimage patches extracted by the multilingual datasets using\\nVOLUME 5, 2017 3199'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nmulti-scale sliding window method. In Ren’s dataset, there\\nare approximately 25,000 training samples being extracted\\nand selected. The number of simple text samples, complex\\ntext samples and background samples is 3000, 6000 and\\n16000, respectively. The text region percentage in a simple\\ntext sample, complex text sample and background sample'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='text sample, complex text sample and background sample\\nis over 80%, 25% to 66% and less than 10%, respectively.\\nIn Pan’s dataset, there are approximately 35,000 training sam-\\nples being extracted and selected, in which 5000 are simple\\ntext samples, 9000 are complex text samples and 21,000 are\\nbackground samples.\\n2) DEEP MODELS\\nThe proposed Chinese text feature extractor is mainly com-\\nposed of one convolutional layer, two convolutional block'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='posed of one convolutional layer, two convolutional block\\nsets and one TSCD block set in sequential. Each block set\\ncontains three blocks. The convolutional layer has 16 ﬁlters\\nwith 3 ×3 window size. The convolutional blocks in the ﬁrst\\nset all have 32 ﬁlters and 3 ×3 window size, while those\\nin the second set all have 64 ﬁlters and the same window\\nsize. The output features of each convolutional block set are\\ndown-sampled by 2 ×2 max-pooling. Each layer in the TSCD'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='down-sampled by 2 ×2 max-pooling. Each layer in the TSCD\\nblocks has 128 ﬁlters in total, which are distributed to eight\\ntext structure component types according to their proportions.\\nThe output features of the TSCD block set are down-sampled\\nby average-pooling with pool-size of 4 ×4 and output as\\nthe extracted features of the proposed Chinese text feature\\nextractor.\\nThe proposed Chinese scene text recognition model is\\ncomposed of the Chinese text feature extractor, a fully con-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='composed of the Chinese text feature extractor, a fully con-\\nnected layer and a softmax classiﬁer in sequential. The fully\\nconnected layer has 2048 units. The softmax classiﬁer has\\n1500 classiﬁcation categories corresponding to the 1500 Chi-\\nnese characters.\\nThe proposed Chinese scene text detection model is also\\ncomposed of the Chinese text feature extractor, a fully con-\\nnected layer and a softmax classiﬁer in sequential. The fully\\nconnected layer has 512 units. The softmax classiﬁer has'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='connected layer has 512 units. The softmax classiﬁer has\\ntwo classiﬁcation categories corresponding to text region and\\nbackground region.\\n3) TRAINING STEPS\\nThe proposed Chinese text feature extractor is trained in the\\nscene text recognition model. And the fully connected layer\\nand the softmax classiﬁer in the recognition model in the\\nrecognition model are trained at the same time. Firstly the\\nrecognition model is trained by using all the character images'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='recognition model is trained by using all the character images\\nin the artiﬁcial part. Then it is further trained by using all the\\nscene character images and 7500 artiﬁcial character images,\\nwhich are randomly selected from the artiﬁcial part and each\\ncharacter has ﬁve samples. The two-step training balances the\\nlarge amount of artiﬁcial character samples and the smaller\\namount but more realistic scene character samples to ensure\\nthe recognition accuracy.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='amount but more realistic scene character samples to ensure\\nthe recognition accuracy.\\nThen the trained Chinese text feature extractor is applied\\nin the scene text detection model to train the fully connected\\nlayer and the softmax classiﬁer of in the model. During\\nthe model training, the parameters in feature extractor are\\nﬁxed and only the fully connected parameters are updated.\\nIn this way, the features extracted by our proposed Chinese'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='In this way, the features extracted by our proposed Chinese\\ntext feature extractor can be used in both text detection and\\nrecognition models as a uniﬁed feature.\\nC. CHINESE SCENE TEXT RECOGNITION\\nThe proposed Chinese text structure feature extractor is com-\\npletely trained in text recognition model. So the performance\\nof the Chinese scene text recognition model is the key evalua-\\ntion of the proposed feature extractor. And the trained feature'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='tion of the proposed feature extractor. And the trained feature\\nextractor will be directly applied to the Chinese scene text\\ndetection model, the quality and accuracy of its extracted fea-\\ntures almost determine the performance of the text detection\\nmodel. Therefore, we ﬁrstly conduct several experiments to\\nevaluate effectiveness of the proposed feature extractor in text\\nrecognition.\\nThe synthetic data engine we designed in Section III.D'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='recognition.\\nThe synthetic data engine we designed in Section III.D\\ngenerates numerous high quality artiﬁcial Chinese character\\nimages for training the text recognition model. Its effective-\\nness is evaluated by training the text recognition model with\\ndifferent training set formations.\\nThe Chinese text recognition model with the proposed\\nChinese text structure feature extractor is evaluated on two\\ntext recognition datasets. One of them is an all natural text'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='text recognition datasets. One of them is an all natural text\\nimage dataset [6] in which all the text images are taken from\\nnatural. The other dataset [40] is a partly natural text image\\ndataset which contains a number of artiﬁcial text images like\\nInternet ADs. Their details are shown in Section IV .A.\\nTABLE 2. Chinese text recognition results.\\nTable II summarizes the Chinese scene text recognition\\nevaluation results with different training set formations and'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='evaluation results with different training set formations and\\ndeep models. The ABBYY , which is a well-known OCR\\nsystem, is evaluated as baseline. ‘‘S,’’ ‘‘A’’ and ‘‘A+S’’ repre-\\nsent different training set formations. ‘‘S’’ represents only the\\nscene character images extracted from the respective training\\nsets are used in training and ‘‘A’’ represents only the artiﬁcial\\ncharacter images generated by the synthetic data engine are\\nused. ‘‘A+S’’ represents the artiﬁcial and scene character'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='used. ‘‘A+S’’ represents the artiﬁcial and scene character\\nimages are all used to train the deep model. The TSCD layer\\nmodel is the Chinese scene text recognition CNN model with\\nTSCD layer in our previous work [39]. The residual network\\nmodel is a normal 20-layer model with three convolutional\\nblock sets, each set contains three blocks. The proposed\\n3200 VOLUME 5, 2017'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nfeature extractor model is the scene text recognition model\\nwith our proposed Chinese text structure feature extractor,\\nwhose detail is shown in Section IV .B.\\nIn the evaluation results, the text recognition model our\\nproposed Chinese text structure feature extractor achieves\\nbetter results in both datasets than other models and the OCR\\nsystem. The proposed feature extractor shows its advantage'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='system. The proposed feature extractor shows its advantage\\nin extracting accurate features for recognition.\\nIt is observed from the results that the text recognition\\nmodels trained with ‘‘S’’ performs much worse than the lead-\\ning results on both datasets, which demonstrates the scene\\ncharacters images extracted from available datasets are far\\nfrom enough to train a creditable deep text recognition model.\\nAnd the models with residual design perform worse than the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='And the models with residual design perform worse than the\\nshallow TSCD layer based model when trained with ‘‘S.’’\\nThese results show that the residual design in deep model is\\nprone to over-ﬁtting with small training set.\\nWhen using the artiﬁcial character images, there is a huge\\ngap between the results on the two datasets. It is because\\nZhou’s dataset contains a number of artiﬁcial text images,\\nwhich is similar to the generated images. So its performance'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='which is similar to the generated images. So its performance\\nhas huge improvement. And as the images in Ren’s dataset\\nare all scene images, the ‘‘A’’ models perform better than the\\n‘‘S’’ models but still much worse than the leading results.\\nModels trained with ‘‘A’’ training set formation have similar\\nrecognition performances. The results of TSCD layer model\\nand residual network model are almost equal. The proposed\\nmodel achieves better recognition accuracies in both dataset,'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='model achieves better recognition accuracies in both dataset,\\nwhich preliminary shows effectiveness of the proposed Chi-\\nnese text structure feature extractor.\\nThe text recognition models trained with ‘‘A+S’’ have\\nconsiderable improvements than ‘‘A,’’ especially in Ren’s\\ndataset, which shows the importance of adding scene charac-\\nter images to train a creditable deep model. Among the mod-\\nels, TSCD layer model and residual network model achieve'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='els, TSCD layer model and residual network model achieve\\nsimilar results on both datasets. The TSCD layer model per-\\nforms slightly better than the residual network model, which\\nindicates priori knowledge structure has larger inﬂuence than\\nvery deep network in text recognition. The model with our\\nproposed Chinese text structure feature extractor achieves\\nmuch better accuracies than the other two models in ‘‘A+S,’’\\nwhich demonstrates that the TSCD layer and the residual'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='which demonstrates that the TSCD layer and the residual\\nnetwork are organically combined in the proposed feature\\nextractor.\\nD. CHINESE SCENE TEXT DETECTION\\nThe scene text detection model with our proposed Chinese\\ntext structure feature extractor is evaluated on two multilin-\\ngual text datasets. Ren’s dataset contains language labels for\\nevery text lines, thus the Chinese text detection model can\\nbe evaluated more accurately by focusing on the Chinese'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='be evaluated more accurately by focusing on the Chinese\\ntext lines. Pan’s dataset does not contain language labels\\nbut has been widely used in many text detection algorithm\\nevaluations. On Pan’s dataset, the text detection model is\\nevaluated in wider language environment and compared to\\nmore text detection algorithms.\\nTABLE 3. Text detection results on Ren’s dataset.\\nTable III summarizes Chinese scene text recognition evalu-\\nation results with different deep models. The CNN model is a'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='ation results with different deep models. The CNN model is a\\nbaseline model contains two convolutional layers, two down-\\nsampling layers, and a fully connected layer. The CSAE\\nmodel uses an unsupervised learning method, which is pro-\\nposed by us in [6], to pretrain the CNN model. The TSCD\\nlayer model has a TSCD layer in place of the second convo-\\nlutional layer in the CNN model. And the CSAE +TSCD\\nlayer model uses the CSAE unsupervised learning method to'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='layer model uses the CSAE unsupervised learning method to\\npretrain the TSCD layer model. The residual network model\\nis a normal 20-layer model with three convolutional block\\nsets, each set contains three blocks. The proposed model is\\nthe scene text detection model with our proposed Chinese\\ntext structure feature extractor, whose detail is shown in\\nSection IV .B. The evaluation method in this experiment is\\nthe same one we used in our previous work [39], which is'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='the same one we used in our previous work [39], which is\\nbased on the evaluation method of ICDAR 2011 and specially\\ndesigned for one-language text detection evaluation.\\nIn the Chinese scene text detection evaluation results of\\ndifferent deep models, the text detection model with our\\nproposed Chinese text structure feature extractor achieves the\\nbest performance of precision 0.87 and recall 0.82. The CSAE\\nunsupervised learning method is applied to extract more accu-'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='unsupervised learning method is applied to extract more accu-\\nrate stroke features by pretraining the model. The residual\\nnetwork is also applied to extract more accurate stroke fea-\\ntures by computing feature residuals. Their evaluation results\\nare similar, but the deconvolution computing in the CSAE is\\nmuch more complex than the residual computing in the resid-\\nual network model. The TSCD layer model also achieves sim-\\nilar overall measurement as the CSAE and residual network'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='ilar overall measurement as the CSAE and residual network\\nmodels do. And it can be noted that it has larger improvement\\nin precision than recall, which demonstrates the uniqueness of\\nChinese text structure features extracted by the TSCD layer.\\nCompared with the model that combines the CSAE method\\nand the TSCD layer, the model with our proposed Chinese\\ntext structure feature extractor achieves better results, even in\\nthe case where the feature extractor is not ﬁne-tuned in the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='the case where the feature extractor is not ﬁne-tuned in the\\ndetection model. It indicates that the design of the proposed\\nfeature extractor has good combination of the TSCD layer\\nand residual network. The results also demonstrate that the\\nextracted Chinese structure features are applicable to both\\ntext detection and recognition.\\nThe proposed Chinese scene text detection model is eval-\\nuated on Pan’s dataset which contains many Chinese text'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='uated on Pan’s dataset which contains many Chinese text\\nregions and has been widely used in multilingual text detec-\\ntion. It enables the proposed model to be compared with more\\ntext detection algorithms in wider language environment.\\nAs most text regions in Pan’s dataset are Chinese text regions,\\nVOLUME 5, 2017 3201'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nFIGURE 7. The Chinese text information extraction result examples.\\nthe performance degradation of the proposed model is lim-\\nited. In this evaluation, the proposed Chinese text struc-\\nture feature extractor in the text detection model is also\\ntrained in the text recognition model in Ren’s dataset and not\\nﬁne-tuned.\\nTABLE 4. Text detection results with different algorithms.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='ﬁne-tuned.\\nTABLE 4. Text detection results with different algorithms.\\nTable IV summarizes the evaluation results of the proposed\\ntext detection model and some text detection algorithms on\\nPan’s dataset. Although many text regions in the dataset\\ndo not contain Chinese texts, which has negative effects\\non both of the measurements, the proposed model achieves\\nthe second best result. Compared to Tian’s algorithm, our\\nmodel has obvious weakness in recall, which is resulted by'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='model has obvious weakness in recall, which is resulted by\\nthe unique Chinese structure we used in the model would\\ncause many English text regions to be detected as background\\nregions. The result implies that the uniqueness of the features\\nextracted by proposed Chinese text structure feature extractor\\nwould not be as effective in other language environments.\\nCompared with Liu’s algorithm, which is the best Chinese\\ntext detection algorithm on Pan’s dataset, our proposed model'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='text detection algorithm on Pan’s dataset, our proposed model\\nachieves better results on both measurements, especially on\\nprecision. The results indicate that the Chinese text structure\\nfeature has advantages in text detection.\\nE. CHINESE SCENE TEXT INFORMATION EXTRACTION\\nWe design a Chinese scene text information extraction sys-\\ntem, which could be regarded as an embryonic form, by sim-\\nply connecting the text detection and recognition models.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='ply connecting the text detection and recognition models.\\nIn this system, a text image is ﬁrstly divided to many image\\npatches by a multi-scale sliding window method. Secondly,\\nthe image patches are classiﬁed by the text detection model.\\nThen the image patches are merged into text lines by sev-\\neral geometric and heuristic rules such as similar colors\\nand horizontal distances. After the text lines are detected,\\nthey are divided to image patches again by a single-scale'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='they are divided to image patches again by a single-scale\\nsliding window method. Then the characters in the text line\\nare recognized individually by the text recognition model.\\nFinally, the characters are connected into recognized text\\nlines according to their positions and recognize scores.\\nIn the evaluation, three types of the embryonic sys-\\ntem, which are constructed with CNN models, TSCD layer\\nmodels and the proposed models, are evaluated on Ren’s'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='models and the proposed models, are evaluated on Ren’s\\ndataset [6] as it contains real and well labeled scene text\\nregions. The system with the proposed models achieves\\nthe best result P/R/F of 0.67/0.58/0.62, compared to the\\nresult of 0.64/0.52/0.57 with the TSCD layer models and\\n0.48/0.39/0.43 with normal CNN models. Some examples\\nof successful results are shown in Fig. 7. It can be noted\\nthat using text structure features (the TSCD layer and the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='that using text structure features (the TSCD layer and the\\nproposed feature extractor), the system performances are\\nsigniﬁcantly improved, which shows the potential of text\\nstructure features in unifying text detection and recognition.\\nAnd the system performers better with the proposed feature\\nextractor than the TSCD layer, which demonstrates the fea-\\ntures extracted by the proposed feature extractor are more\\naccurate than those extracted by TSCD layer based models.\\nV. CONCLUSION'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='accurate than those extracted by TSCD layer based models.\\nV. CONCLUSION\\nIn this paper, we present a novel Chinese text structure fea-\\nture extractor for both text detection and recognition. It well\\nsimulates the key mechanisms in the three-layer Chinese\\ntext cognition model of human, which enables human detect\\nand recognize texts at the same time. In the Chinese text\\nstructure feature extractor, text structure component detector\\nlayer, which simulates the structure component layer in the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='layer, which simulates the structure component layer in the\\npsychological model, and residual network, which simulates\\nthe bidirectional information transmission in the psycholog-\\nical model, are combined by a TSCD block. The design of\\nTSCD block incorporates the design ideas of the TSCD layer\\nand the residual network, which not only integrates them into\\na whole, but also enables them to complement each other.\\nTherefore, the features extracted by the TSCD block based'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='Therefore, the features extracted by the TSCD block based\\nChinese text structure feature extractor are applicable to both\\nChinese text detection and recognition as we human do.\\nExperimental results demonstrate that the proposed Chinese\\ntext structure feature extractor is effective in both Chinese\\nscene text detection and recognition due to its high accuracy\\nand completeness in Chinese text structure feature extraction.\\nIt is also observed that adding artiﬁcial training samples'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='It is also observed that adding artiﬁcial training samples\\nis essential in training the proposed feature extractor in a\\ntext recognition model as the existing training samples are\\nlimited. The feature extractor trained in the text recognition\\nmodel is effective in a text detection model even it is not ﬁne-\\ntuned for detection. And an embryonic form of Chinese scene\\ntext information extraction system using the feature extrac-\\ntor in both stages achieves promising results. Therefore, the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='tor in both stages achieves promising results. Therefore, the\\n3202 VOLUME 5, 2017'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nproposed feature extractor is suitable to Chinese text infor-\\nmation extraction algorithms as a uniﬁed feature extractor for\\nboth text detection and recognition.\\nREFERENCES\\n[1] X. Chen and A. L. Yuille, ‘‘Detecting and reading text in natural scenes,’’ in\\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), vol. 2. Jul. 2004,\\npp. II-366–II-373.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), vol. 2. Jul. 2004,\\npp. II-366–II-373.\\n[2] X. Li, W. Wang, S. Jiang, Q. Huang, and W. Gao, ‘‘Fast and effective text\\ndetection,’’ inProc. 15th IEEE Int. Conf. Image Process. (ICIP), Oct. 2008,\\npp. 969–972.\\n[3] C. Jung, Q. Liu, and J. Kim, ‘‘A stroke ﬁlter and its application to\\ntext localization,’’ Pattern Recognit. Lett., vol. 30, no. 2, pp. 114–122,\\n2009.\\n[4] B. Epshtein, E. Ofek, and Y . Wexler, ‘‘Detecting text in natural scenes'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='2009.\\n[4] B. Epshtein, E. Ofek, and Y . Wexler, ‘‘Detecting text in natural scenes\\nwith stroke width transform,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. (CVPR), Jun. 2010, pp. 2963–2970.\\n[5] Q. Ye, Q. Huang, W. Gao, and D. Zhao, ‘‘Fast and robust text detec-\\ntion in images and video frames,’’ Image Vis. Comput., vol. 23, no. 6,\\npp. 565–576, Jun. 2005.\\n[6] X. Ren, K. Chen, X. Yang, Y . Zhou, J. He, and J. Sun, ‘‘A new unsupervised'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='[6] X. Ren, K. Chen, X. Yang, Y . Zhou, J. He, and J. Sun, ‘‘A new unsupervised\\nconvolutional neural network model for chinese scene text detection,’’\\nin Proc. IEEE China Summit Int. Conf. Signal Inf. Process. (ChinaSIP),\\nJul. 2015, pp. 428–432.\\n[7] W. Huang, Y . Qiao, and X. Tang, ‘‘Robust scene text detection with\\nconvolution neural network induced MSER trees,’’ in Proc. Eur. Conf.\\nComput. Vis. (ECCV), 2014, pp. 497–511.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Comput. Vis. (ECCV), 2014, pp. 497–511.\\n[8] L. Wu, P. Shivakumara, T. Lu, and C. L. Tan, ‘‘A new technique for multi-\\noriented scene text line detection and tracking in video,’’ IEEE Trans.\\nMultimedia, vol. 17, no. 8, pp. 1137–1152, Jan. 2015.\\n[9] A. Mishra, K. Alahari, and C. Jawahar, ‘‘Scene text recognition using\\nhigher order language priors,’’ in Proc. Brit. Mach. Vis. Conf. (BMVC),\\n2012, p. 1.\\n[10] C. Yao, X. Bai, B. Shi, and W. Liu, ‘‘Strokelets: A learned multi-scale'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='2012, p. 1.\\n[10] C. Yao, X. Bai, B. Shi, and W. Liu, ‘‘Strokelets: A learned multi-scale\\nrepresentation for scene text recognition,’’ in Proc. IEEE Conf. Comput.\\nVis. Pattern Recognit. (CVPR), Jun. 2014, pp. 4042–4049.\\n[11] I. J. Goodfellow, Y . Bulatov, J. Ibarz, S. Arnoud, and V . Shet,\\n(2013). ‘‘Multi-digit number recognition from street view imagery\\nusing deep convolutional neural networks,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1312.6082'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='using deep convolutional neural networks,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1312.6082\\n[12] C. Shi, C. Wang, B. Xiao, Y . Zhang, S. Gao, and Z. Zhang, ‘‘Scene\\ntext recognition using part-based tree-structured character detection,’’\\nin Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Apr. 2013,\\npp. 2961–2968.\\n[13] T. Novikova, O. Barinova, P. Kohli, and V . Lempitsky, ‘‘Large-lexicon\\nattribute-consistent text recognition in natural images,’’ in Proc. Eur. Conf.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='attribute-consistent text recognition in natural images,’’ in Proc. Eur. Conf.\\nComput. Vis. (ECCV), 2012, pp. 752–765.\\n[14] K. Wang, B. Babenko, and S. Belongie, ‘‘End-to-end scene text recogni-\\ntion,’’ in Proc. Comput. Vis. (ICCV), 2011, pp. 1457–1464.\\n[15] L. Neumann and J. Matas, ‘‘On combining multiple segmentations\\nin scene text recognition,’’ in Proc. 12th Int. Conf. Document Anal.\\nRecognit. (ICDAR), Oct. 2013, pp. 523–527.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Recognit. (ICDAR), Oct. 2013, pp. 523–527.\\n[16] T. Wang, D. J. Wu, A. Coates, and A. Y . Ng, ‘‘End-to-end text recognition\\nwith convolutional neural networks,’’ in Proc. 21st Int. Conf. Pattern\\nRecognit. (ICPR), Nov. 2012, pp. 3304–3308.\\n[17] C. Yao, X. Bai, and W. Liu, ‘‘A uniﬁed framework for multioriented text\\ndetection and recognition,’’ IEEE Trans. Image Process., vol. 23, no. 11,\\npp. 4737–4749, Jun. 2014.\\n[18] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman, ‘‘Reading'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='pp. 4737–4749, Jun. 2014.\\n[18] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman, ‘‘Reading\\ntext in the wild with convolutional neural networks,’’ Int. J. Comput. Vis.,\\nvol. 116, no. 1, pp. 1–20, 2016.\\n[19] Y . Zhu, C. Yao, and X. Bai, ‘‘Scene text detection and recognition:\\nRecent advances and future trends,’’ Frontiers Comput. Sci., vol. 10, no. 1,\\npp. 19–36, 2016.\\n[20] P. Shivakumara, W. Huang, T. Q. Phan, and C. L. Tan, ‘‘Accurate video text'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='pp. 19–36, 2016.\\n[20] P. Shivakumara, W. Huang, T. Q. Phan, and C. L. Tan, ‘‘Accurate video text\\ndetection through classiﬁcation of low and high contrast images,’’ Pattern\\nRecognit., vol. 43, no. 6, pp. 2165–2185, Jun. 2010.\\n[21] P. Shivakumara, T. Q. Phan, and C. L. Tan, ‘‘A Laplacian approach to multi-\\noriented text detection in video,’’ IEEE Trans. Pattern Anal. Mach. Intell.,\\nvol. 33, no. 2, pp. 412–419, Feb. 2011.\\n[22] H. Chen, S. S. Tsai, G. Schroth, D. M. Chen, R. Grzeszczuk, and'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='[22] H. Chen, S. S. Tsai, G. Schroth, D. M. Chen, R. Grzeszczuk, and\\nB. Girod, ‘‘Robust text detection in natural images with edge-enhanced\\nmaximally stable extremal regions,’’ in Proc. 18th IEEE Int. Conf. Image\\nProcess. (ICIP), Sep. 2011, pp. 2609–2612.\\n[23] H. Cho, M. Sung, and B. Jun, ‘‘Canny text detector: Fast and robust scene\\ntext localization algorithm,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. (CVPR), Mar. 2016, pp. 3566–3573.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Recognit. (CVPR), Mar. 2016, pp. 3566–3573.\\n[24] C. Yao, X. Bai, W. Liu, Y . Ma, and Z. Tu, ‘‘Detecting texts of arbitrary\\norientations in natural images,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. (CVPR), Oct. 2012, pp. 1083–1090.\\n[25] L. Kang, Y . Li, and D. Doermann, ‘‘Orientation robust text line detec-\\ntion in natural images,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. (CVPR), Mar. 2014, pp. 4034–4041.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Recognit. (CVPR), Mar. 2014, pp. 4034–4041.\\n[26] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu, and X. Bai, (2016). ‘‘Multi-\\noriented text detection with fully convolutional networks,’’ [Online]. Avail-\\nable: https://arxiv.org/abs/1604.04018\\n[27] Q. Ye and D. Doermann, ‘‘Text detection and recognition in imagery:\\nA survey,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 7,\\npp. 1480–1500, Jul. 2015.\\n[28] C.-Y . Lee and S. Osindero, (2016). ‘‘Recursive recurrent nets with'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='pp. 1480–1500, Jul. 2015.\\n[28] C.-Y . Lee and S. Osindero, (2016). ‘‘Recursive recurrent nets with\\nattention modeling for OCR in the wild,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1603.03101\\n[29] B. Shi, X. Wang, P. Lv, C. Yao, and X. Bai, (2016). ‘‘Robust\\nscene text recognition with automatic rectiﬁcation,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1603.03915\\n[30] M. Jaderberg, A. Vedaldi, and A. Zisserman, ‘‘Deep features for text'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='[30] M. Jaderberg, A. Vedaldi, and A. Zisserman, ‘‘Deep features for text\\nspotting,’’ in Proc. Eur. Conf. Comput. Vis., 2014, pp. 512–528.\\n[31] X. Ren, K. Chen, X. Yang, Y . Zhou, J. He, and J. Sun, ‘‘A novel text struc-\\nture feature extractor for chinese scene text detection and recognition,’’ in\\nProc. 23nd Int. Conf. Pattern Recognit. (ICPR), Aug. 2016, pp. 36–45.\\n[32] K. He, X. Zhang, S. Ren, and J. Sun, (2015). ‘‘Deep residual learning for'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='[32] K. He, X. Zhang, S. Ren, and J. Sun, (2015). ‘‘Deep residual learning for\\nimage recognition,’’ [Online]. Available: https://arxiv.org/abs/1512.03385\\n[33] M. Jaderber, K. Simonyan, A. Vedaldi and A. Zisserman. (2014). ‘‘Syn-\\nthetic data and artiﬁcial neural networks for natural scene text recogni-\\ntion.’’ [Online]. Available: https://arxiv.org/abs/1406.2227\\n[34] A. Gupta, A. Vedaldi, and A. Zisserman, (2016). ‘‘Synthetic\\ndata for text localisation in natural images,’’ [Online]. Available:'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='data for text localisation in natural images,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1604.06646\\n[35] M. Taft and X. Zhu, ‘‘The representation of bound morphemes in the lexi-\\ncon: A Chinese study,’’ in Proc. Morphol. Aspects Lang. Process. (MALP),\\n1995, pp. 293–316.\\n[36] M. Shen, Z. Li, and Z. Zuxiang, ‘‘The inﬂuence of component startup on\\nthe recognition of ﬁtting Chinese characters,’’ Psychol. Sci., vol. 3, no. 5,\\npp. 206–211, 1997.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='the recognition of ﬁtting Chinese characters,’’ Psychol. Sci., vol. 3, no. 5,\\npp. 206–211, 1997.\\n[37] J. Liu, S. Zhang, H. Li, and W. Liang, ‘‘A Chinese character localization\\nmethod based on intergrating structure and CC-clustering for advertis-\\ning images,’’ in Proc. Int. Conf. Document Anal. Recognit. (ICDAR),\\nSep. 2011, pp. 1044–1048.\\n[38] X. Zhou and Y . Li, ‘‘A research of chinese character utility function (in\\nchinese),’’Linguistic Res., vol. 1, no. 10, pp.62–65, 2009.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='chinese),’’Linguistic Res., vol. 1, no. 10, pp.62–65, 2009.\\n[39] X. Ren, Y . Zhou, J. He, K. Chen, X. Yang, and J. Sun, ‘‘A convolutional\\nneural network based Chinese text detection algorithm via text struc-\\nture modeling,’’ IEEE Trans. Multimedia, vol. 19, no. 3, pp. 506–518,\\nMar. 2017.\\n[40] X. Zhou, S. Zhou, C. Yao, Z. Cao, and Q. Yin, (2015). ‘‘Icdar\\n2015 text reading in the wild competition,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1506.03184'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='2015 text reading in the wild competition,’’ [Online]. Available:\\nhttps://arxiv.org/abs/1506.03184\\n[41] Y .-F. Pan, X. Hou, and C.-L. Liu, ‘‘A hybrid approach to detect and localize\\ntexts in natural scene images,’’ IEEE Trans. Image Process., vol. 20, no. 3,\\npp. 800–813, Mar. 2011.\\n[42] A. Shahab, F. Shafait, and A. Dengel, ‘‘ICDAR 2011 robust reading\\ncompetition challenge 2: Reading text in scene images,’’ in Proc. Int. Conf.\\nDocument Anal. Recognit. (ICDAR), Mar. 2011, pp. 1491–1496.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Document Anal. Recognit. (ICDAR), Mar. 2011, pp. 1491–1496.\\n[43] X.-C. Yin, X. Yin, K. Huang, and H.-W. Hao, ‘‘Robust text detection in\\nnatural scene images,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 36,\\nno. 5, pp. 970–983, Oct. 2014.\\n[44] S. Tian, Y . Pan, C. Huang, S. Lu, K. Yu, and C. L. Tan, ‘‘Text ﬂow: A\\nuniﬁed text detection system in natural scene images,’’ in Proc. IEEE Int.\\nConf. Comput. Vis. (ICCV), Dec. 2015, pp. 4651–4659.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 4651–4659.\\n[45] X. Liu, Z. Lu, J. Li, and W. Jiang, ‘‘Detection and segmentation text from\\nnatural scene images based on graph model,’’ in Proc. WSEAS Trans.\\nSignal Process., 2014, p. 10.\\nVOLUME 5, 2017 3203'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='X. Renet al.: Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition\\nXIAOHANG REN received the B.S. degree in\\nelectronic engineering from Zhejiang University,\\nHangzhou, China, in 2011. He is currently pur-\\nsuing the Ph.D. degree with the Department of\\nElectronic Engineering, Shanghai Jiao Tong Uni-\\nversity, Shanghai, China. His research interests\\ninclude text information extraction, deep learning\\nnetwork, and image retrieving.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='include text information extraction, deep learning\\nnetwork, and image retrieving.\\nYI ZHOUreceived the Ph.D. degree from Shang-\\nhai Jiao Tong University, China, in 2010. She is\\ncurrently with the Computer Science Department,\\nShanghai Jiaotong University, China. Her project\\nof Chinese characters reconginition is supported\\nby the National Science Foundation. Her major\\nresearch includes object recognition and big data\\nmining.\\nZHENG HUANG received the Ph.D. degree in'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='research includes object recognition and big data\\nmining.\\nZHENG HUANG received the Ph.D. degree in\\ncomputer application technology from Shanghai\\nJiao Tong University, China, in 2003. He was\\na Computer Application Technology Professional\\nDoctorate with Shanghai Jiao Tong University\\nin 2003, where he is currently an Associate Pro-\\nfessor with the Institute of Information Security\\nand Engineering and also with the School of Infor-\\nmation Security Engineering. His research area is'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='mation Security Engineering. His research area is\\ninformation security and research interests include\\ncryptography and machine learning, information security, cryptography, and\\nmachine learning.\\nJUN SUN (M’06) received the B.S. degree in\\nelectrical engineering from the University of\\nElectronic Sciences and Technology of China,\\nChengdu, China, in 1989, and the Ph.D. degree\\nin electrical engineering from Shanghai Jiao Tong\\nUniversity, in 1995. In 1996, he was elected'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='in electrical engineering from Shanghai Jiao Tong\\nUniversity, in 1995. In 1996, he was elected\\nas a member of the HDTV Technical Executive\\nExperts Group, China. Since then, he has been\\nacting as one of the main technical experts for the\\nChinese government in the ﬁeld of digital televi-\\nsion and multimedia communications. In the past ﬁve years, he has been\\nresponsible for several national projects in DTV and IPTV ﬁelds. He is'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='responsible for several national projects in DTV and IPTV ﬁelds. He is\\ncurrently a Professor and Ph.D. Advisor with Shanghai Jiao Tong University.\\nHe has authored or co-authored over 50 technical papers in the area of\\ndigital television and multimedia communications. His research interests\\ninclude digital television, multimedia communication, and video encoding.\\nHe received the Second Prize of the National Science and Technology\\nDevelopment Award in 2003 and 2008.'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Development Award in 2003 and 2008.\\nXIAOKANG YANG (SM’14) received the\\nB.S. degree from Xiamen University, Xia-\\nmen, China, in 1994, the M.S. degree from\\nthe Chinese Academy of Sciences, Shang-\\nhai, China, in 1997, and the Ph.D. degree\\nfrom Shanghai Jiao Tong University, Shang-\\nhai, in 2000. From 2000 to 2002, he was a\\nResearch Fellow with the Center for Signal\\nProcessing, Nanyang Technological University,\\nSingapore. From 2002 to 2004, he was a Research'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Processing, Nanyang Technological University,\\nSingapore. From 2002 to 2004, he was a Research\\nScientist with the Institute for Infocomm Research, Singapore. From 2007 to\\n2008, he visited the Institute for Computer Science, University of Freiburg,\\nGermany, as an Alexander von Humboldt Research Fellow. He is currently\\na Distinguished Professor with the School of Electronic Information and\\nElectrical Engineering and also the Deputy Director of the Institute of Image'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Electrical Engineering and also the Deputy Director of the Institute of Image\\nCommunication and Information Processing, Shanghai Jiao Tong Univer-\\nsity. He has authored or co-authored over 200 refereed papers. He holds\\n40 patents. His current research interests include visual signal processing\\nand communication, media analysis and retrieval, and pattern recognition.\\nHe is a member of the Asia-Paciﬁc Signal and Information Processing'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='He is a member of the Asia-Paciﬁc Signal and Information Processing\\nAssociation, a member of the Visual Signal Processing and Communications\\nTechnical Committee of the IEEE Circuits and Systems Society, a member\\nof the Multimedia Signal Processing Technical Committee of the IEEE\\nSignal Processing Society, the Chair of the Multimedia Big Data Interest\\nGroup of Multimedia Communications Technical Committee of the IEEE\\nCommunication Society. He was a member of the Editorial Board of the'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='Communication Society. He was a member of the Editorial Board of the\\nDigital Signal Processing. He is also an Associate Editor of the IEEE\\nSIGNAL PROCESSING LETTERS and the Series Editor of the Communications in\\nComputer and Information Science(Springer).\\nKAI CHEN(M’09) received the Ph.D. degree from\\nShanghai Jiao Tong University, China, in 2003.\\nHe is currently with the Institute of Image Com-\\nmunication and Network Engineering, Shanghai\\nJiao Tong University. His major research includes'),\n",
       " Document(metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content='munication and Network Engineering, Shanghai\\nJiao Tong University. His major research includes\\ninformation retrieving, object recognition, and big\\ndata mining. He is the Key Member of the Insti-\\ntute on Network Engineering Research. He has\\nbeen involved with several key nation projects\\nand hosted many Industry-Academia Research\\nprojects.\\n3204 VOLUME 5, 2017')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_split = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "final_split = text_split.split_documents(doc)\n",
    "final_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a32dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='stages to remove the error propagation between them, leading\n",
      "to signiﬁcant improvements.\n",
      "However, existing works still have certain limitations.\n",
      "In particular, most of them only focus on English texts,\n",
      "which are relatively easy to deﬁne and recognize due to the\n",
      "simplicity in strokes and structures. In this age of globaliza-\n",
      "tion, the recognition of multilingual texts attracts increasing\n",
      "VOLUME 5, 2017\n",
      "2169-3536' metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "page_content='tion, the recognition of multilingual texts attracts increasing\n",
      "VOLUME 5, 2017\n",
      "2169-3536 \n",
      " 2017 IEEE. Translations and content mining are permitted for academic research only.\n",
      "Personal use is also permitted, but republication/redistribution requires IEEE permission.\n",
      "See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n",
      "3193' metadata={'producer': 'MiKTeX pdfTeX-1.40.13', 'creator': 'TeX', 'creationdate': '2017-03-22T15:43:47+05:30', 'trapped': '/False', 'moddate': '2017-03-27T06:53:33-04:00', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4535 (1.40.13)', 'source': 'sample.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(final_split[11])\n",
    "print()\n",
    "print(final_split[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b47ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'sample.txt'}, page_content=' Scenetextinformationextractionplaysanimportantroleinmanycomputervisionapplications.\\n Most features in existing text extraction algorithms are only applicable to one text extraction stage (text\\n detection or recognition), which signi cantly weakens the consistency in an end-to-end system, especially\\n for the complex Chinese texts. \\n To tackle this challenging problem, we propose a novel text structure feature\\n extractor based on a text structure component detector (TSCD) layer and residual network for Chinese texts.\\n Inspired by the three-layer Chinese text cognition model of a human, we combine the TSCD layer and\\n the residual network to extract features suitable for both text extraction stages. The specialized modeling\\n for Chinese characters in the TSCD layer simulates the key structure component cognition layer in the\\n psychological model. And the residual mechanism in the residual network simulates the key bidirectional\\n connection among the layers in the psychological model. Through the organic combination of the TSCD\\n layer and the residual network, the extracted features are applicable to both text detection and recognition,\\n as humans do. \\n In evaluation, both text detection and recognition models based on our proposed text structure\\n feature extractor achieve great improvements over baseline CNN models. And an end-to-end Chinese text\\n information extraction system is experimentally designed and evaluated, showing the advantage of the\\n proposed feature extractor as a uni ed feature extractor.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('sample.txt')\n",
    "txt = loader.load()\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368cbf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Most features in existing text extraction algorithms are only applicable to'\n",
      "page_content='are only applicable to one text extraction stage (text'\n"
     ]
    }
   ],
   "source": [
    "txt=\"\"\n",
    "with open('sample.txt') as f:\n",
    "    txt = f.read()\n",
    "    \n",
    "text_split = RecursiveCharacterTextSplitter(chunk_size=80, chunk_overlap=30)\n",
    "text = text_split.create_documents([txt])\n",
    "print(text[2])\n",
    "print(text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d648733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
